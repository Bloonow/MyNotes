[toc]

# 五、计算机组成原理和结构

前面章节介绍的计算机指令系统结构从软件的角度描述了计算机功能，从本章开始将介绍计算机组成结构，从硬件的角度来看看计算机是怎样构成的。

如果说图灵机是现代计算机的计算理论模型，冯诺依曼结构就是现代计算机的结构理论模型。本章从冯诺依曼的理论模型开始，介绍计算机系统的各个组成部分，并与现代计算机的具体实现相对应。

## （一）冯诺依曼结构

现代计算机都采用存储程序结构，又称为冯诺依曼结构，是世界上第一个完整的计算机体系结构。冯诺依曼结构的主要特点是：(1)计算机由存储器、运算器、控制器、输入设备和输出设备五部分组成，其中运算器和控制器合称为中央处理器（Central Processing Processor，CPU）。(2)存储器是按地址访问的线性编址的一维结构，每个单元的位数固定。(3)采用存储程序方式，即指令和数据不加区别混合存储在同一个存储器中。(4)控制器通过执行指令发出控制信号控制计算机的操作。指令在存储器中按其执行顺序存放，由指令计数器指明要执行的指令所在的单元地址。指令计数器一般按顺序递增，但执行顺序可按运算结果或当时的外界条件而改变。(5)以运算器为中心，输入输出设备与存储器之间的数据传送都经过运算器。

冯诺依曼计算机的工作原理如下图5.1所示。

<img src="计算机硬件结构.assets/图5.1 冯诺伊曼计算机体系结构.png" style="zoom:50%;" />

随着技术的进步，冯诺依曼结构得到了持续的改进，主要包括以下几个方面：

1. 由以运算器为中心改进为以存储器为中心。使数据的流向更加合理，从而使运算器、存储器和输入输出设备能够并行工作。
2. 由单一的集中控制改进为分散控制。计算机发展初期，工作速度很低，运算器、存储器、控制器和输入输出设备可以在同一个时钟信号的控制下同步工作。现在运算器、内存与输入输出设备的速度差异很大，需要采用异步方式分散控制。
3. 从基于串行算法改进为适应并行算法。出现了流水线处理器、超标量处理器、向量处理器、多核处理器、对称多处理器（Symmetric Multi-Processor，SMP）、大规模并行处理机（Massively Parallel Processing，MPP）和机群系统等。
4. 出现为适应特殊需要的专用计算机，如图形处理器（Graphic Processing Unit，GPU）、数字信号处理器（Digital Signal Processor，DSP）等。
5. 在非冯诺依曼计算机的研究方面也取得一些成果，如依靠数据驱动的数据流计算机、图归约计算机等。

虽然经过了长期的发展，现代计算机系统占据主要地位的仍然是以存储程序和指令驱动执行为主要特点的冯诺依曼结构。在通用计算机领域，由于应用软件的多样性，要求计算机不断地变化所执行的程序内容，并且频繁地对数据与程序占用的存储器资源进行重新分配，使用统一编址可以最大限度地利用资源。

而作为冯诺依曼结构的一个变种，哈佛结构把程序和数据分开存储。控制器使用两条独立的总线读取程序和访问数据，程序空间和数据空间完成分开。在嵌入式应用中，系统要执行的任务相对单一，程序一般是固化在硬件里的，同时嵌入式系统对安全性、可靠性的要求更高，哈佛结构独立的程序空间更有利于代码保护。因此，在嵌入式领域，哈佛结构得到了广泛应用。需要指出的是，哈佛结构并没有改变冯诺依曼结构存储程序和指令驱动执行的本质，它只是冯诺依曼结构的一个变种，并不是独立于冯诺依曼结构的一种新型结构。

## （二）计算机的组成部件

本节对计算机的主要组成部件进行介绍。按照冯诺依曼结构，计算机包含五大部分，即运算器、控制器、存储器、输入设备和输出设备。

### 1. 运算器

运算器是计算机中负责计算（包括算术计算和逻辑计算等）的部件，包括算术和**逻辑运算部件（Arithmetic Logic Unit，ALU)**、移位部件、**浮点运算部件（Floating Point Unit，FPU）**、向量运算部件、寄存器等。其中，复杂运算如乘除法、开方及浮点运算可用程序实现或由运算器实现。寄存器既可用于保存数据，也可用于保存地址。运算器还可设置条件码寄存器、状态寄存器等专用寄存器，条件码寄存器保存当前运算结果的状态，如运算结果是正数、负数或零，是否溢出等。

运算器支持的运算类型经历了从简单到复杂的过程。最初的运算器只有简单的定点加减和基本逻辑运算，复杂运算如乘除通过加减、移位指令构成的数学库完成；后来逐渐出现硬件定点乘法器和除法器。在早期的微处理器中，浮点运算器以协处理器的形式出现在计算机中（如Intel 8087协处理器），包含二进制浮点数的加、减、乘、除等运算，**现代的通用微处理器则普遍包含完整的浮点运算部件**。20世纪90年代开始，微处理器中出现了单指令多数据（Single Instruction Multiple Data，SIMD）的向量运算器，部分处理器还实现了超越函数硬件运算单元，如sin、cos、exp、log等。部分用于银行业务处理的计算机（如IBM Power系列）还实现了十进制定点数、浮点数的运算器。

随着晶体管集成度的不断提升，处理器中所集成的运算器的数量也持续增加，通常将具有相近属性的一类运算组织在一起构成一个运算单元。不同的处理器有不同的运算单元组织，有的倾向于每个单元大而全，有的倾向于每个单元的功能相对统一。处理器中包含的运算单元数目也逐渐增加，从早期的单个运算单元逐渐增加到多个运算单元。由于运算单元都需要从寄存器中读取操作数，并把结果写回寄存器，因此处理器中运算单元的个数主要受限于寄存器堆的读写端口个数。运算单元一般按照定点、浮点、访存、向量等大类来组织，也有混合的，如SIMD部件既能做定点也能做浮点运算，定点部件也可以做访存地址计算等。

下表5.1给出了几种经典处理器的运算器结构。其中Alpha 21264、MIPS R10000、HP PA8700、Ultra Sparc III、Power 4是20世纪90年代RISC处理器鼎盛时期经典的微处理器，而Intel Skylake、AMD Zen、Power8、龙芯3A5000则是最新处理器。

<img src="计算机硬件结构.assets/表5.1 经典处理器的运算器结构.png" style="zoom:50%;" />

### 2. 控制器

控制器是计算机中发出控制命令以控制计算机各部件自动、协调地工作的装置。控制器控制指令流和每条指令的执行，内含程序计数器（Program Counter，PC）和指令寄存器（Instruction Register，IR）等。程序计数器存放当前执行指令的地址，指令寄存器存放当前正在执行的指令。指令通过译码产生控制信号，用于控制运算器、存储器、IO设备的工作以及后续指令的获取。这些控制信号可以用硬连线逻辑产生，也可以用微程序产生，也可以两者结合产生。为了获得高指令吞吐率，可以采用指令重叠执行的流水线技术，以及同时执行多条指令的超标量技术。当遇到执行时间较长或条件不具备的指令时，把条件具备的后续指令提前执行（称为乱序执行）可以提高流水线效率。控制器还产生一定频率的时钟脉冲，用于计算机各组成部分的同步。

由于控制器和运算器的紧密耦合关系，现代计算机通常把控制器和运算器集成在一起，称为中央处理器，即CPU。随着芯片集成度的不断提高，现代CPU除了含有运算器和控制器外，常常还集成了其他部件，比如高速缓存（Cache）部件、内存控制器等。

计算机执行指令一般包含以下过程：从存储器取指令并对取回的指令进行译码，从存储器或寄存器读取指令执行需要的操作数，执行指令，把执行结果写回存储器或寄存器。上述过程称为一个指令周期。计算机不断重复指令周期直到完成程序的执行。体系结构研究的一个永恒主题就是不断加速上述指令执行周期，从而提高计算机运行程序的效率。控制器负责控制指令流和每条指令的执行，对提高指令执行效率起着至关重要的作用。

现代处理器的控制器都通过指令流水线技术来提高指令执行效率。指令流水线把一条指令的执行划分为若干阶段（如分为取指、译码、执行、访存、写回阶段）来减少每个时钟周期的工作量，从而提高主频；并允许多条指令的不同阶段重叠执行实现并行处理（如一条指令处于执行阶段时，另一条指令处于译码阶段）。虽然同一条指令的执行时间没有变短，但处理器在单位时间内执行的指令数增加了。

计算机中的取指部件、运算部件、访存部件都在流水线的调度下具体执行指令规定的操作。运算部件的个数和延迟，访存部件的存储层次、容量和带宽，以及取指部件的转移猜测算法等是决定微结构性能的重要因素。常见的提高流水线效率的技术包括转移预测技术、乱序执行技术、超标量（又称为多发射）技术等。

(1) 转移预测技术。冯诺依曼结构指令驱动执行的特点，使转移指令成为提高流水线效率的瓶颈。典型应用程序平均每5\~10条指令中就有一条转移指令，而转移指令的后续指令需要等待转移指令执行结果确定后才能取指，导致转移指令和后续指令之间不能重叠执行，降低了流水线效率。随着主频的提高，**现代处理器流水线普遍在10\~20级之间**，由于转移指令引起的流水线阻塞成为提高指令流水线效率的重要瓶颈。

转移预测技术可以消除转移指令引起的指令流水线阻塞。转移预测器根据当前转移指令或其他转移指令的历史行为，在转移指令的取指或译码阶段预测该转移指令的跳转方向和目标地址并进行后续指令的取指。转移指令执行后，根据已经确定的跳转方向和目标地址对预测结果进行修正。如果发生转移预测错误，还需要取消指令流水线中的后续指令。为了提高预测精度并降低预测错误时的流水线开销，现代高性能处理器采用了复杂的转移预测器。

(2) 乱序执行（动态调度）技术。如果指令I是条长延迟指令，如除法指令或Cache不命中的访存指令，那么在顺序指令流水线中指令I后面的指令需要在流水线中等待很长时间。乱序执行技术通过指令动态调度允许指令I后面的源操作数准备好的指令越过指令I执行，以提高指令流水线效率；而需要使用指令I的运算结果的指令由于源操作数没有准备好，不会越过指令I执行。

为此，在指令译码之后的读寄存器阶段，应判断指令需要的操作数是否准备好。如果操作数已经准备好，就进入执行阶段；如果操作数没有准备好，就进入称为保留站（或称为发射队列）的队列中等待，直到操作数准备好后再进入执行阶段。为了保证执行结果符合程序规定的要求，乱序执行的指令需要有序结束。为此，执行完的指令均进入一个称为**重排序缓冲（Re-Order Buffer，ROB）**的队列，并把执行结果临时写入重命名寄存器。ROB根据指令进入流水线的次序，有序提交指令的执行结果到目标寄存器或存储器。CDC6600和IBM 360/91分别使用记分板和保留站最早实现了指令的动态调度。

就像保留站和重排序缓冲用来临时存储指令以使指令在流水线中流动更加通畅，重命名寄存器用来临时存储数据以使数据在流水线流动更加通畅。保留站、重排序缓冲、重命名寄存器都是微结构中的数据结构，程序员无法用指令来访问，是结构设计人员为了提高流水线效率而用来临时存储指令和数据的。其中，保留站把指令从有序变为无序以提高执行效率，重排序缓存把指令从无序重新变为有序以保证正确性，重命名寄存器则在乱序执行过程中临时存储数据。重命名寄存器与指令可以访问的结构寄存器（如通用寄存器、浮点寄存器）相对应。乱序执行流水线把指令执行结果写入重命名寄存器而不是结构寄存器，以避免破坏结构寄存器的内容，到顺序提交阶段再把重命名寄存器内容写入结构寄存器。两组执行不同运算但使用同一结构寄存器的指令可以使用不同的重命名寄存器，从而避免该结构寄存器成为串行化瓶颈，实现并行执行。

(3) 超标量（多发射）技术。工艺技术的发展使得在20世纪80年代后期出现了超标量处理器。超标量结构允许指令流水线的每一阶段同时处理多条指令。例如Alpha 21264处理器每拍可以取4条指令，发射6条指令，写回6条指令，提交11条指令。如果把单发射结构比作单车道马路，多发射结构就是多车道马路。

由于超标量结构的指令和数据通路都变宽了，使得寄存器端口、保留站端口、ROB端口、功能部件数都需要增加，例如Alpha 21264的寄存器堆有8个读端口和6个写端口，数据Cache的RAM通过倍频支持一拍两次访问。现代超标量处理器一般包含两个以上访存部件，两个以上定点运算部件以及两个以上浮点运算部件。超标量结构在指令译码或寄存器重命名时不仅要判断前后拍指令的数据相关，还需要判断同一拍中多条指令间的数据相关。

### 3. 存储器

存储器存储程序和数据，又称主存储器或内存，一般用**动态随机访问存储器（Dynamic Random Access Memory，RAM）**实现。CPU可以直接访问它，IO设备也频繁地与它交换数据。存储器的存取速度往往满足不了CPU的快速要求，容量也满足不了应用的需要，为此将存储系统分为高速缓存（Cache）、主存储器和辅助存储器三个层次。Cache存放当前CPU最频繁访问的部分主存储器内容，可以采用比DRAM速度快但容量小的**静态随机访问存储器（Static Random Access Memory，SRAM）**实现。数据和指令在Cache和主存储器之间的调动由硬件自动完成。为扩大存储器容量，使用磁盘、磁带、光盘等能存储大量数据的存储器作为辅助存储器。计算机运行时所需的应用程序、系统软件和数据等都先存放在辅助存储器中，在运行过程中分批调入主存储器。数据和指令在主存储器和辅助存储器之间的调动由操作系统完成。CPU访问存储器时，好像面对的是一个高速（接近于Cache的速度）、大容量（接近于辅助存储器的容量）的存储器。

现代计算机中还有少量**只读存储器（Read Only Memory，ROM）**用来存放引导程序和**基本输入输出系统（Basic Input Output System，BIOS）**等。现代计算机访问内存时采用虚拟地址，操作系统负责维护虚拟地址和物理地址转换的页表，集成在CPU中的**存储管理部件（Memory Management Unit，MMU）**负责把虚拟地址转换为物理地址。

存储器的主要评价指标为存储容量和访问速度。存储容量越大，可以存放的程序和数据越多。访问速度越快，处理器访问的时间越短。对相同容量的存储器，速度越快的存储介质成本越高，而成本越低的存储介质则速度越低。目前人们发明的用于计算机系统的存储介质主要包括以下几类：

(1) 磁性存储介质。如硬盘、磁带等，特点是存储密度高、成本低、具有非易失性（断电后数据可长期保存），缺点是访问速度慢。磁带的访问速度在秒级，磁盘的访问速度一般在毫秒级，这样的访问速度显然不能满足现代处理器纳秒级周期的速度要求。

(2) 闪存（Flash Memory）。同样是非易失性的存储介质，与磁盘相比，它们的访问速度快，成本高，容量小。随着闪存工艺技术的进步，闪存芯片的集成度不断提高，成本持续降低，闪存正在逐步取代磁盘作为计算机尤其是终端的辅助存储器。

(3) 动态随机访问存储器（DRAM）。属于易失性存储器（断电后数据丢失）。特点是存储密度较高（存储一位数据只需一个晶体管），需要周期性刷新，访问速度较快。其访问速度一般在几十纳秒级。

(4) 静态随机访问存储器（SRAM）。属于易失性存储器（断电后数据丢失）。存储密度不如DRAM高（SRAM存储一位数据需要4\~8个晶体管），不用周期性刷新，但访问速度比DRAM快，可以达到纳秒级，小容量时能够和处理器核工作在相同的时钟频率。

现代计算机中把上述不同的存储介质组成存储层次，以在成本合适的情况下降低存储访问延迟，如下图5.2中所示，越往上的层级，速度越快，但成本越高，容量越小；越往下的层级，速度越慢，但成本越低，容量越大。

<img src="计算机硬件结构.assets/图5.2 存储层次.png" style="zoom:50%;" />

图5.2所示存储层次中的寄存器和主存储器直接由指令访问，Cache缓存主存储器的部分内容；而非易失存储器既可以是辅助存储器，又可以是输入输出设备，非易失存储器的内容由操作系统负责调入调出主存储器。

存储层次的有效性，依赖于程序的访存局部性原理，包含两个方面：一是时间局部性，指的是如果一个数据被访问，那么在短时间内很有可能被再次访问；二是空间局部性，指的是如果一个数据被访问，那么它的邻近数据也很有可能被访问。利用局部性原理，可以把程序近期可能用到的数据存放在靠上的层次，把近期内不会用到的数据存放在靠下的层次。通过恰当地控制数据在层次间的移动，使处理器需要访问的数据尽可能地出现在靠近处理器的存储层次，可以大大提高处理器获得数据的速度，从而近似达到“用最快的存储器构建一个容量很大的单级存储”的效果。

现代计算机一般使用多端口寄存器堆实现寄存器，使用SRAM来构建片上的高速缓存（Cache），使用DRAM来构建程序的主存储器（也称为主存、内存），使用磁盘或闪存来构建大容量的存储器。

#### (1) 高速缓存

随着工艺技术的发展，处理器的运算速度和内存容量按摩尔定律的预测指数增加，但内存速度提高非常缓慢，与处理器速度的提高形成了“剪刀差”。工艺技术的上述特点使得访存延迟成为以存储器为中心的冯诺依曼结构的主要瓶颈。Cache技术利用程序访问内存的时间局部性和空间局部性，使用速度较快、容量较小的Cache临时保存处理器常用的数据，使得处理器的多数访存操作可以在Cache上快速进行，只有少量访问Cache不命中的访存操作才访问内存。

Cache是内存的映像，其内容是内存内容的子集，处理器访问Cache和访问内存使用相同的地址。从20世纪80年代开始，RISC处理器就开始在处理器芯片内集成KB级的小容量Cache。**现代处理器则普遍在片内集成多级Cache，典型的多核处理器的每个处理器核中一级指令Cache和数据Cache各几十KB，二级Cache为几百KB，而多核共享的三级Cache为几MB到几十MB。现代处理器访问寄存器时一拍可以同时读写多个数据，访问一级Cache延迟为1\~4拍，访问二级Cache延迟为10\~20拍，访问三级Cache延迟为40\~60拍，访问内存延迟为100\~200拍。**

CPU执行一个程序的时间可以描述为程序中的**指令数÷IPC×时钟周期**。其中IPC（Instruction Per Cycle）表示每个时钟周期执行的指令数，它可以细分为运算指令的IPC×运算指令的比例+访存指令的IPC×访存指令的比例。访存指令的IPC为**平均访问延迟AMAT（Average Memory Access Latency）**的倒数。在具有高速缓存的计算机中，其值如下：
$$
\text{AMAT}=\text{HitTime}+\text{MissRate}\times\text{MissPenalty}
$$
其中HitTime表示高速缓存命中时的访问延迟，MissRate表示高速缓存失效率，MissPenalty表示高速缓存失效时额外的访问延迟。例如，在某计算机系统中$\text{HitTime}=1,\text{MissRate}=5\%,\text{MissPenalty}=100$，则$\text{AMAT}=1+5\%\times 100=6$。

#### (2) 内存

主存储器又称为内存。内存的读写速度对计算机的整体性能影响重大。为了提升处理器的访存性能，**现代通用处理器都将内存控制器与CPU集成在同一芯片内**，以减小平均访存延迟。

现代计算机的内存一般都采用同步动态随机存储器（SDRAM）实现。DRAM的一个单元由MOS管T和电容C（存储单元）组成，如下图5.3所示。

<img src="计算机硬件结构.assets/图5.3 DRAM的单元读写原理.png" style="zoom:50%;" />

电容C存储的电位决定存储单元的逻辑值。单元中的字线根据读写地址译码得到，连接同一字的若干位；单元中的位线把若干字的同一位链接在一起。进行读操作时，先把位线预充到$V_{ref}=V_{CC}/2$，然后字线打开T管，C引起差分位线微小的电位差，感应放大器读出，读出后C中的电位被破坏，需要把读出值重新写入C。进行写操作时，先把位线预充成要写的值，然后打开字线，把位线的值写入C。需要注意的是，C中的电容可能会漏掉，因此DRAM需要周期刷新，刷新可以通过读操作进行，一般每行几十微秒刷新一次。

SDRAM芯片一般采用行列地址线复用技术，对SDRAM进行读写时，需要先发送行地址打开一行，再发送列地址读写需要访问的存储单元。为了提高访问的并发度，SDRAM芯片一般包含多个Bank（存储块），这些Bank可以并行操作。下图5.4显示了一个DDR2 SDRAM×8芯片的内部结构图。可以看到，该SDRAM内部包含了8个Bank，每个Bank对应一个存储阵列和一组感应放大器，所有的Bank共用读锁存（Read Latch）和写FIFO队列。

<img src="计算机硬件结构.assets/图5.4 SDRAM的功能结构图.png" style="zoom:50%;" />

对SDRAM进行写操作后，由于必须等到写数据从IO引脚传送到对应Bank的感应放大器后，才能进行后续的预充电操作（针对相同Bank）或者读操作（针对所有Bank），因此写操作会给后续的其他操作带来较大的延迟，但连续的写操作却可以流水执行。为了降低写操作带来的开销，内存控制器往往将多个写操作聚集在一起连续发送，以分摊单个写操作的开销。

影响SDRAM芯片读写速度的因素有两个：行缓冲局部性（Row Buffer Locality，RBL）和Bank级并行度（Bank Level Parallelism，BLP）。

(1) 行缓冲局部性。如图5.4所示，SDRAM芯片的一行数据在从存储体中读出后，存储体中的值被破坏，保存在对应的一组感应放大器中，这组感应放大器也被称为行缓冲。如果下一个访存请求访问同一行的数据（称为命中行缓冲），可以直接从该感应放大器中读出，而不需要重新访问存储体内部，可以大大降低SDRAM的访问延迟。当然，在行缓冲不命中的时候，就需要首先将行缓冲中的数据写回存储体，再将下一行读出到行缓冲中进行访问。

由此，对DRAM可以采用关行（Close Page）和开行（Open Page）两种策略。使用关行策略时，每次读写完后先把行缓冲的内容写入存储体，才能进行下一次读写，每次读写的延迟是确定的。使用开行策略时，每次读写完后不把行缓冲的内容写入存储体，如果下一次读写时所读写的数据在行缓冲中（称为行命中），可以直接对行缓冲进行读写即可，延迟最短；如果下一次读写时所读写的数据不在行缓冲中，则需要先将行缓冲中的数据写回对应的行，再将新地址的数据读入行缓冲，再进行读写，延迟最长。因此，如果内存访问的局部性好，可以采用开行策略；如果内存访问的局部性不好，则可以采用关行策略。内存控制器可以通过对多个访存请求进行调度，尽量把对同一行的访问组合在一起，以增加内存访问的局部性。

(2) Bank级并行度。SDRAM芯片包含的多个Bank是相互独立的，它们可以同时执行不同的操作，比如，对Bank 0激活的同时，可以对Bank 1发出预充电操作，因此，访问不同Bank的多个操作可以并行执行。Bank级并行度可以降低冲突命令的等待时间，容忍单个Bank访问的延迟。

利用内存的这两个特性，可以在内存控制器上对并发访问进行调度，尽可能降低读写访问的平均延迟，提高内存的有效带宽。内存控制器可以对十几甚至几十个访存请求进行调度，有效并发的访存请求数越多，可用于调度的空间就越大，可能得到的访存性能就更优。

### 4. 输入/输出设备

输入/输出设备（简称IO设备）实现计算机与外部世界的信息交换。传统的IO设备有键盘、鼠标、显示器和打印机等；新型的IO设备能进行语音、图像、影视的输入、输出和手写体文字输入，并支持计算机之间通过网络进行通信。磁盘等辅助存储器在计算机中也当作IO设备来管理。

处理器通过读写IO设备控制器中的寄存器来访问及控制IO设备。高速IO设备可以在处理器安排下直接与主存储器成批交换数据，称为**直接存储器访问（Directly Memory Access，DMA）**。处理器可以通过查询设备控制器状态与IO设备进行同步，也可以通过中断与IO设备进行同步。

下面以GPU、硬盘和闪存为例介绍典型的IO设备。

#### (1) GPU

**图形处理单元（Graphics Processing Unit，GPU）**是与CPU联系最紧密的外设之一，主要用来处理2D和3D的图形、图像和视频，以支持基于视窗的操作系统、图形用户界面、视频游戏、可视化图像应用和视频播放等。当在电脑上打开播放器观看电影时，GPU负责将压缩后的视频信息解码为原始数据，并通过显示控制器显示到屏幕上；当拖动鼠标移动一个程序窗口时，GPU负责计算移动过程中和移动后的图像内容；当运行游戏时，GPU负责计算并生成游戏画面。

GPU驱动提供OpenGL、DirectX等应用程序编程接口以方便图形编程。其中，OpenGL是一个用于3D图形编程的开放标准；DirectX是微软公司推出的一系列多媒体编程接口，包括用于3D图形的Direct 3D。通过这些应用程序接口，软件人员可以很方便地实现功能强大的图形处理软件，而不必关心底层的硬件细节。

GPU最早是作为一个独立的板卡出现的，所以称为显卡。我们常说的独立显卡和集成显卡是指GPU是作为一个独立的芯片出现还是被集成在芯片组或处理器中。现代GPU内部包含了大量的计算单元，可编程性越来越强，除了用于图形图像处理外，也越来越多地用作高性能计算的加速部件，称为加速卡。

GPU与CPU之间存在大量的数据传输。CPU将需要显示的原始数据放在内存中，让GPU通过DMA的方式读取数据，经过解析和运算，将结果写至显存中，再由显示控制器读取显存中的数据并输出显示。将GPU与CPU集成至同一个处理器芯片时，CPU与GPU内存一致性维护的开销和数据传递的延迟都会大幅降低。此时系统内存需要承担显存的任务，访存压力也会大幅增加，因为图形应用具有天生的并行性，GPU可以轻松地耗尽有限的内存带宽。

GPU的作用是对图形API定义的流水线实现硬件加速，主要包括以下几个阶段：

1. 顶点读入（Vertex Fetch）：从内存或显存中取出顶点信息，包括位置、颜色、纹理坐标、法向量等属性。
2. 顶点着色（Vertex Shader）：对每一个顶点进行坐标和各种属性的计算。
3. 图元装配（Primitive Assembly）：将顶点组合成图元，如点、线段、三角形等。
4. 光栅化（Rasterization）：将矢量图形点阵化，得到被图元覆盖的像素点，并计算属性插值系数以及深度信息。
5. 片元着色（Fragment Shader）：进行属性插值，计算每个像素的颜色。
6. 逐片元操作（Per-Fragment Operation）：进行模板测试、深度测试、颜色混合和逻辑操作等，并最终修改渲染缓冲区。

在GPU中，集成了专用的硬件电路来实现特定功能，同时也集成了大量可编程的计算处理核心用于一些较为通用的功能实现。设计者根据每个功能使用的频率、方法以及性能要求，选择不同的实现方式。大部分GPU中，顶点读入、图元装配、光栅化及帧缓冲操作使用专用硬件电路实现，而顶点着色和片元着色采用可编程的计算处理核心实现。由于现代GPU中集成了大量可编程的计算处理核心，这种大规模并行的计算模式经过改进后非常适合科学计算应用，所以在高性能计算机领域，GPU常被用作计算加速单元配合CPU使用。

#### (2) 硬盘

计算机除了需要内存存放程序的中间数据外，还需要具有永久记忆功能的存储体来存放需要较长时间保存的信息。比如操作系统的内核代码、文件系统、应用程序和用户的文件数据等。该存储器除了容量必须足够大之外，价格还要足够便宜，同时速度还不能太慢。在计算机的发展历史上，磁性存储材料正好满足了以上要求。磁性材料具有断电记忆功能，可以长时间保存数据；磁性材料的存储密度高，可以搭建大容量存储系统；同时，磁性材料的成本很低。

人们目前使用的硬盘就是一种磁性存储介质。硬盘的构造原理为：将磁性材料覆盖在圆形碟片（或者说盘片）上，通过一个读写头（磁头）悬浮在碟片表面来感知存储的数据，通过碟片的旋转和磁头的径向移动来读写碟片上任意位置的数据。碟片被划分为多个环形的轨道（称为磁道，Track）来保存数据，每个磁道又被分为多个等密度（等密度数据）的弧形扇区（Sector）作为存储的基本单元。磁盘的内部构造如下图5.5所示。

<img src="计算机硬件结构.assets/图5.5 磁盘的内部结构示意图.png" style="zoom:50%;" />

硬盘在工作时，盘片是一直旋转的，当想要读取某个扇区的数据时，首先要将读写头移动到该扇区所在的磁道上，当想要读写的扇区旋转到读写头下时，读写头开始读写数据。

衡量磁盘性能的指标包括响应时间和吞吐量，也就是延迟和带宽。磁头移动到目标磁道的时间称为寻道时间；当磁头移动到目标磁道后，需要等待目标扇区旋转到磁头下面，这段时间称为旋转时间。旋转时间与盘片的旋转速度有关，磁盘的旋转速度用RPM（Rotation Per Minute，转/分）来表示。扇区旋转到目标位置后，传输这个扇区的数据同样需要时间，称为传输时间。传输时间是扇区大小、旋转速度和磁道记录密度的函数。

磁盘是由磁盘控制器控制的。磁盘控制器控制磁头的移动、接触和分离以及磁盘和内存之间的数据传输。另外，通过IO操作访问磁盘控制器又会引入新的时间。现在的磁盘内部一般都会包含一个数据缓冲，读写磁盘时，如果访问的数据正好在缓冲中命中，则不需要访问磁盘扇区。还有，当有多个命令读写磁盘时，还需要考虑排队延迟。因此，磁盘的访问速度计算起来相当复杂。一般来说，磁盘的平均存取时间在几个毫秒的量级。

磁盘的密度一直在持续增加，对于用户来说，磁盘的容量一直在不断增大。磁盘的尺寸也经历了一个不断缩小的过程，从最大的14英寸到最小的1.8英寸。目前市场上常见的磁盘尺寸包括应用于台式机的3.5英寸和应用于笔记本电脑的2.5英寸。

#### (3) 闪存

闪存（Flash Storage）是一种半导体存储器，它和磁盘一样是非易失性的存储器，但是它的访问延迟却只有磁盘的千分之一到百分之一，而且它尺寸小、功耗低，抗震性更好。常见的闪存有SD卡、U盘和SSD固态磁盘等。与磁盘相比，闪存的每GB价格较高，因此容量一般相对较小。目前闪存主要应用于移动设备中，如移动电话、数码相机、MP4播放器，主要原因在于它的体积较小。闪存在移动市场具有很强的应用需求，工业界投入了大量财力推动闪存技术的发展。随着技术的发展，闪存的价格在快速下降，容量在快速增加，因此SSD固态硬盘技术获得了快速发展。SSD固态硬盘是使用闪存构建的大容量存储设备，它模拟硬盘接口，可以直接通过硬盘的SATA总线与计算机相连。

最早出现的闪存被称为NOR型闪存，因为它的存储单元与一个标准的或非门很像。NAND型闪存采用另一种技术，它的存储密度更高，每GB的成本更低，因此NAND型闪存适合构建大容量的存储设备。前面所列的SD卡、U盘和SSD固态硬盘一般都是用NAND型闪存构建的。

使用闪存技术构建的永久存储器存在一个问题，即闪存的存储单元随着擦写次数的增多存在损坏的风险。为了解决这个问题，大多数NAND型闪存产品内部的控制器采用地址块重映射的方式来分布写操作，目的是将写次数多的地址转移到写次数少的块中。该技术被称为磨损均衡（Wear Leveling）。闪存的平均擦写次数在10万次左右。这样，通过磨损均衡技术，移动电话、数码相机、MP3播放器等消费类产品在使用周期内就不太可能达到闪存的写次数限制。闪存产品内部的控制器还能屏蔽制造过程中损坏的块，从而提高产品的良率。

## （三）计算机系统硬件结构发展

前面章节从冯诺依曼结构出发，介绍了现代计算机的理论结构及其组成部分。随着应用需求的变化和工艺水平的不断提升，冯诺依曼结构中的控制器和运算器逐渐演变为计算机系统中的中央处理器部分，而输入、输出设备统一通过北桥和南桥与中央处理器连接，中央处理器中的图形处理功能则从中央处理器中分化出来形成专用的图形处理器。因此，**现代计算机系统的硬件结构主要包括了中央处理器、图形处理器、北桥及南桥等部分**。

中央处理器（Central Processing Unit，CPU）主要包含控制器和运算器，在发展的过程中不断与其他部分融合。传统意义上的中央处理器在处理器芯片中更多地体现为处理器核，**现代的处理器芯片上往往集成多个处理器核**。

图形处理器（Graphic Processing Unit，GPU）是一种面向2D和3D图形、视频、可视化计算和显示优化的处理器。作为人机交互的重要界面，GPU在计算机体系结构发展的过程中，担任了越来越重要的角色。除了对图形处理本身之外，还开始担负科学计算加速器的任务。

北桥（North Bridge）是离CPU最近的芯片，主要负责控制显卡、内存与CPU之间的数据交换，向上连接处理器，向下连接南桥。

南桥（South Bridge）主要负责硬盘、键盘以及各种对带宽要求较低的IO接口与内存、CPU之间的数据交换。

### 1. CPU-GPU-北桥-南桥四片结构

现代计算机的一种早期结构是CPU-GPU-北桥-南桥结构。在该结构中，计算机系统包含四个主要芯片，其中CPU（处理器）芯片、北桥芯片和南桥芯片一般是直接以芯片的形式安装或焊接在计算机主板上，而GPU则以显卡的形式安装在计算机主板的插槽上。

在CPU-GPU-北桥-南桥四片结构中，计算机的各个部件根据速度快慢以及与处理器交换数据的频繁程度被安排在北桥和南桥中。CPU通过处理器总线（也称系统总线）和北桥直接相连，北桥再通过南北桥总线和南桥相连，GPU一般以显卡的形式连接北桥。内存控制器集成在北桥芯片中，硬盘接口、USB接口、网络接口、音频接口以及鼠标、键盘等接口放在南桥芯片中。此外，在北桥上还会提供各种扩展接口用于其他功能卡的连接。采用该结构的微机系统如下图5.6所示。

<img src="计算机硬件结构.assets/图5.6 CPU-GPU-北桥-南桥结构.png" style="zoom:50%;" />

与英特尔奔腾处理器搭配的430HX芯片组就采用了这样的四片结构。其北桥芯片使用82439HX，南桥芯片采用82371SB，通过PCI总线扩展外接显卡，与处理器组成四片结构，作为计算机系统的主要部分。

### 2. CPU-北桥-南桥三片结构

在CPU-北桥-南桥结构中，系统包含三个主要芯片，分别为CPU芯片、北桥芯片和南桥芯片。三片结构与四片结构最大的区别是，前者GPU功能被集成到北桥，即一般所说的集成显卡。

在CPU-北桥-南桥三片结构中，CPU通过处理器总线和北桥直接相连，北桥再通过南北桥总线和南桥相连。内存控制器、显示功能以及高速IO接口（如PCIE等）集成在北桥芯片中，硬盘接口、USB接口、网络接口、音频接口以及鼠标、键盘等接口部件放在南桥芯片中。随着计算机技术的发展，更多的高速接口被引入计算机体系结构中，在北桥上集成的IO接口的速率也不断提升。采用该结构的微机系统如下图5.7所示。

<img src="计算机硬件结构.assets/图5.7 CPU-北桥-南桥结构.png" style="zoom:50%;" />

英特尔845G芯片组就采用类似的三片结构。其北桥芯片使用82845G，集成显示接口，南桥芯片采用82801DB，与处理器组成三片结构，作为计算机系统的主要部分。

### 3. CPU-弱北桥-南桥三片结构

随着工艺和设计水平的不断提高，芯片的集成度不断提高，单一芯片中能够实现的功能越来越复杂。内存接口的带宽需求超过了处理器与北桥之间连接的处理器总线接口带宽，导致内存的实际访问性能受限于处理器总线的性能。而伴随着处理器核计算性能的大幅提升，存储器的性能提升却显得幅度较小，这两者的差异导致计算机系统性能受到存储器系统发展的制约，这就是存储墙问题。

因此，对计算机系统性能影响显著的内存控制器开始被集成到CPU芯片中，从而大幅降低了内存访问延迟，提升了内存访问带宽，这在一定程度上缓解了存储墙问题。于是，北桥的功能被弱化，主要集成了GPU、显示接口、高速IO接口（例如PCIE接口等）。采用该结构的微机系统如下图5.8所示。

<img src="计算机硬件结构.assets/图5.8 CPU-弱北桥-南桥结构.png" style="zoom:50%;" />

相比英特尔，AMD的处理器最早将内存控制器集成到处理器芯片中，780E芯片组就采用上述三片结构，北桥芯片使用RS780E，集成HD3200 GPU，南桥芯片使用SB710，与处理器组成三片结构，作为计算机系统的主要部分。

### 4. CPU-南桥两片结构

在计算机系统不断发展的过程中，图形处理器性能也在飞速发展，其在系统中的作用也不断被开发出来。除了图形加速以外，对于一些科学计算类的应用，或者是一些特定的算法加速程序，图形处理器发挥着越来越大的作用，成为特定的运算加速器，其与中央处理器之间的数据共享也越来越频繁，联系越来越密切。

随着芯片集成度的进一步提高，图形处理器也开始被集成到CPU芯片中，于是，北桥存在的必要性就进一步降低，开始和南桥合二为一，形成CPU-南桥结构，如下图5.9所示。

<img src="计算机硬件结构.assets/图5.9 CPU-南桥结构.png" style="zoom:50%;" />

在这个结构中，CPU芯片集成处理器核、内存控制器和GPU等主要部件，对外提供显示接口、内存接口等，并通过处理器总线和南桥相连。南桥芯片则包含硬盘、USB、网络控制器以及PCIE/PCI、LPC等总线接口。由于GPU和CPU都需要大量访问内存，会带来一些访存冲突，而且相对来说，GPU对于实时性的要求更高，即访存优先级会更高一些，这在一定程度上会影响CPU的性能。实际上，处理器中集成的GPU性能相比独立显卡中的GPU性能会稍弱。

当然，也有一些两片结构是将GPU集成在南桥芯片中。这样在南桥上可以实现独立的显存供GPU使用，这在某些条件下更有利于GPU性能的发挥，且CPU升级时带来的开销会更小。

### 5. SoC单片结构

片上系统（System on Chip，SoC）是一种单片计算机系统解决方案，它在单个芯片上集成了处理器、内存控制器、GPU以及硬盘、USB、网络等IO接口，使得用户搭建计算机系统时只需要使用单个主要芯片即可，如下图5.10所示。

<img src="计算机硬件结构.assets/图5.10 SoC单片结构.png" style="zoom:50%;" />

目前SoC主要应用于移动处理器和工业控制领域，相比上述几种多片结构，单片SoC结构的集成度更高，功耗控制方法更加灵活，有利于系统的小型化和低功耗设计。但也因为全系统都在一个芯片上实现，导致系统的扩展性没有多片结构好，升级的开销也更大。随着技术的发展，封装基板上的互连技术不断发展和成熟，越来越多的处理器利用多片封装技术在单个芯片上集成多个硅片，以扩展芯片的计算能力或IO能力。

例如AMD Ryzen（锐龙）系列处理器通过在封装上集成多个处理器硅片和IO硅片，以提供针对不同应用领域的计算能力。龙芯3C5000L处理器则通过在封装上集成4个4核龙芯3A5000硅片来实现单片16核结构。

目前，**主流商用处理器中面向中高端领域的处理器普遍采用两片结构**，而面向中低端及嵌入式领域的处理器普遍采用单片结构。SoC单片结构最常见的是在手机等移动设备中。

## （四）处理器和IO设备间的通信

前面介绍了组成计算机系统的各个部分，在冯诺依曼结构中，处理器（准确地说是内部的控制器）处于中心位置，需要控制其他各个部件的运行。对存储器的控制是通过读写指令来完成的。存储器是存储单元阵列，对某个地址的读写不会影响其他存储单元。

而IO设备大都是具有特定功能的部件，不能当作简单的存储阵列来处理。由于IO设备的底层控制相当复杂，它们一般都是由一个设备控制器进行控制。设备控制器会提供一组寄存器接口，寄存器的内容变化会引起设备控制器执行一系列复杂的动作。设备控制器的接口寄存器也被称为IO寄存器。处理器通过读写IO寄存器来访问设备。写入这些寄存器的数据，会被设备控制器解析成命令，因此有些情况下将处理器对IO寄存器的访问称为命令字。

处理器对内存和IO的访问模式有所不同，对访问的延迟和带宽需求也有较大差异。现代计算机系统的程序和数据都存放在内存中，内存访问性能直接影响处理器流水线的执行效率，也正是因为这样，才导致了各个Cache层次的出现。对处理器的内存访问来说，要求是高带宽和低延迟。IO设备一般用于外部交互，而IO操作一般会要求顺序的访问控制，从而导致执行效率低下，访问带宽低，延迟高，只能通过IO的DMA操作来提升性能。IO的DMA操作也是访问内存，因为DMA访存模式一般是大块的连续数据读写，所以对带宽的需求远高于对延迟的需求。

### 1. IO寄存器寻址

为了访问IO寄存器，处理器必须能够寻址这些寄存器。IO寄存器的寻址方式有两种：内存映射IO和特殊IO指令。

内存映射IO是把IO寄存器的地址映射到内存地址空间中，这些寄存器和内存存储单元被统一编址。读写IO地址和读写内存地址使用相同的指令来执行。处理器需要通过它所处的状态来限制应用程序可以访问的地址空间，使其不能直接访问IO地址空间，从而保证应用程序不能直接操作IO设备。与内存映射IO不同，特殊IO指令使用专用指令来执行IO操作。因此，IO地址空间可以和内存地址空间重叠，但实际指向不同的位置。操作系统可以通过禁止应用程序执行IO指令的方式来阻止应用程序直接访问IO设备。

MIPS或LoongArch结构并没有特殊IO指令，通过普通的访存指令访问特定的内存地址空间进行IO访问。而X86结构使用专门的IO指令来执行IO操作。

### 2. 处理器和IO设备之间的同步

处理器和IO设备之间需要协同工作，通过一系列软件程序来共同发挥设备功能。处理器和IO设备之间的同步有两种方式：查询和中断。

处理器通过向IO寄存器写入命令字来控制IO设备。大部分的控制操作不是通过一次寄存器写入就能完成的，处理器一般需要对IO寄存器进行多次访问，才能完成一次任务。绝大多数设备的IO寄存器不是无条件写入的，处理器在写入命令字之前，先要获取设备的当前状态，只有当设备进入特定的状态后，处理器才能执行特定的操作，这些特定的软件操作流程是在驱动程序中实现的。

比如，对于一台打印机，打印机控制器会提供两个寄存器：数据寄存器和状态寄存器。数据寄存器用来存放当前需要打印的数据，状态寄存器用来指示打印机的状态，它包含两个基本位，完成位和错误位。完成位表示上一个字符打印完毕，可以打印下一个字符；错误位用来在打印机出现异常时指示出错的状态，比如卡纸或者缺纸。处理器在打印一串数据时，首先把数据写入数据寄存器，然后不断读取状态寄存器的值，当读出的完成位等于1时，才能把下一个字符写入数据寄存器。同时，处理器还需要检查错误位的值，当发生错误时，去执行对应的错误处理程序。

前面描述的打印过程就是查询方式的一个例子。当使用查询方式时，处理器向IO设备发出访问请求后，需要不断读取IO设备的状态寄存器，所以查询方式也被称为轮询。由于IO设备的速度一般都较慢，使用查询方式会浪费处理器的指令周期。而且，执行轮询的处理器无法同时执行其他工作，造成了性能的浪费。

为了解决查询方式效率较低的问题，中断方式被引入计算机系统。在中断方式下，处理器不需要轮询状态寄存器的值，而是在等待设备完成某个操作的过程中转去执行其他进程。当设备完成某个操作后，自行产生一个中断信号来中断处理器的执行。处理器被中断后，再去读取设备的状态寄存器。中断方式将处理器从等待IO中解放了出来，大大提高了处理器的利用率，因此现代计算机的绝大部分IO设备都支持中断方式。

中断本质上是IO设备对处理器发出的一个信号，让处理器知道此时有数据传输需要或者已经发生数据传输。CPU收到中断信号后，会暂停当前CPU的执行进程，转去执行某个特定的程序。中断的一般过程为：

1. 中断源发出中断信号到中断控制器；
2. 中断控制器产生中断请求给CPU；
3. CPU发出中断响应，并读取中断类型码；
4. CPU根据中断类型码执行对应的中断服务程序；
5. CPU从中断服务程序返回，中断结束。

中断源即中断的源头，比如用户敲击一下键盘，单击一下鼠标，或者DMA的一次传输完成了，对应的控制器会产生一个中断信号。中断信号可以是一根信号线，也可以是一个消息包。这个中断信息会传送到中断控制器中。中断控制器是负责中断汇集、记录和转发的硬件逻辑。中断控制器一般都具有可编程功能，因此被称为可编程中断控制器（Programmable Interrupt Controller，PIC）。

典型的中断控制器如Intel的8259A，它支持中断嵌套和中断优先级，可以支持8个中断源，并可以通过级联的方式进行扩展。8259A内部包含3个寄存器：中断请求寄存器（Interrupt Request Register，IRR），用来存放当前的中断请求；中断在服务寄存器（In-Service Register，ISR），用来存放当前CPU正在服务的中断请求；中断屏蔽寄存器（Interrupt Mask Register，IMR），用来存放中断屏蔽位。

当中断源产生中断信号后，会将中断请求寄存器的某一位设置为1，如果该位没有被屏蔽，则产生一个中断信号（比如中断线）给处理器。处理器检测到该中断信号，并跳转到固定的地址执行中断服务例程。在中断服务例程中，处理器通过读取8259A获得中断向量号，进而调用对应的中断服务程序。在中断服务程序返回之前，要保证本次中断的中断信号被清除掉，否则CPU从中断服务程序返回之后，会被再次触发中断。8259A在中断响应时会自动将IRR的对应位复位。对于电平触发的中断，中断服务程序一般会读写中断源的相关寄存器，从而保证在中断返回之前，中断源的中断信号被撤掉，这样8259A的中断请求寄存器的对应位不会被再次置位。对于脉冲触发的中断，则不需要对设备IO寄存器进行处理。

### 3. 存储器和IO设备之间的数据传送

存储器和IO设备之间需要进行大量的数据传输。例如，系统在启动时，需要把操作系统代码从硬盘搬运到内存中；计算机想要输出图形时，需要把准备显示的数据从内存搬运到显示控制器中。

早期，存储器和IO设备之间的数据传送都是由处理器来完成的。由于存储器和IO设备之间没有直接的数据通路，当需要从存储器中搬运数据到IO设备时，处理器首先从存储器中读数据到通用寄存器中，再从通用寄存器写数据到IO设备中；当需要从IO设备搬运数据到存储器中时，处理器要先从IO设备中读数据到通用寄存器，再从通用寄存器写入内存。这种方式称为PIO（Programming Input/Output）模式。

由于IO访问的访问延迟一般较大，而且IO访问之间需要严格的顺序关系，因而PIO方式的带宽较低。PIO模式存在两种同步方式：查询方式和中断方式。虽然中断方式可以降低处理器查询的开销，但当进行大量数据传输时，PIO模式仍然需要占用大量的处理器时间。使用中断方式，每传送一定的数据后都要进入一次中断，在中断服务程序中真正用于数据传送的时间可能并不多，大量的时间被用于断点保护、中断向量查询、现场恢复、中断返回等辅助性工作。对于一些数据传送速率较快的设备，PIO方式可能会因为处理器搬运数据速度较慢而降低数据的传送速度，因此PIO方式一般用于键盘、鼠标等低速设备。

在PIO方式中，数据要经过处理器内部的通用寄存器进行中转。中转不仅影响处理器的执行，也降低了数据传送的速率。如果在存储器和IO设备之间开辟一条数据通道，专门用于数据传输，就可以将处理器从数据搬运中解放出来。这种方式就是**直接存储器访问（Direct Memory Access，DMA）**方式。DMA方式在存储器和外设之间开辟直接的数据传送通道，数据传送由专门的硬件来控制。控制DMA数据传送的硬件被称为DMA控制器。

使用DMA进行传输的一般过程为：

1. 处理器为DMA请求预先分配一段地址空间。
2. 处理器设置DMA控制器参数。这些参数包括设备标识、数据传送的方向、内存中用于数据传送的源地址或目标地址、传输的字节数量等。
3. DMA控制器进行数据传输。DMA控制器发起对内存和设备的读写操作，控制数据传输。DMA传输相当于用IO设备直接续写内存。
4. DMA控制器向处理器发出一个中断，通知处理器数据传送的结果（成功或者出错以及错误信息）。
5. 处理器完成本次DMA请求，可以开始新的DMA请求。

DMA方式对于存在大量数据传输的高速设备是一个很好的选择，**硬盘、网络、显示等设备普遍都采用DMA方式**。一个计算机系统中通常包含多个DMA控制器，比如有特定设备专用的SATA接口DMA控制器、USB接口DMA控制器等，也有通用的DMA控制器用于可编程的源地址与目标地址之间的数据传输。

DMA控制器的功能可以很简单，也可以很复杂。例如，DMA控制器可以仅仅支持对一段连续地址空间的读写，也可以支持对多段地址空间的读写以及执行其他的IO操作。不同IO设备的DMA行为各不相同，因此现代的IO控制器大多会实现专用的DMA控制器用于自身的数据传输。

下表5.2举例说明了PIO和DMA两种数据传输方式的不同。

<img src="计算机硬件结构.assets/表5.2 PIO和DMA两种数据传输方式.png" style="zoom:50%;" />

从上面两个例子中可以看到，PIO方式和DMA方式处理的流程一致，区别在于：首先键盘的数据是被记录在IO设备本身的，而网卡的数据则直接由网卡写入内存之中；其次CPU处理时，对键盘是直接从IO寄存器读数据，而对网卡则直接从内存读数据。

看起来似乎差别不大。但需要考虑的是，IO访问相比内存访问慢很多，而且对于内存访问，CPU可以通过Cache、预取等方式进行加速，IO访问则缺少这种有效的优化方式。在上面的例子中，如果网卡采用PIO的方式使用CPU，对网卡的包一个字一个字地进行读访问，效率将非常低下。而对于键盘来说，一次输入仅仅只有8位数据，而且相比处理器的处理速度，键盘输入的速度相当低，采用PIO的处理方式能够很简单地完成数据输入任务。

### 4. 龙芯3A3000+7A1000桥片系统中的CPU、GPU、DC通信

下面以龙芯3A3000+7A1000桥片中CPU、GPU、DC间的同步与通信为例说明处理器与IO间的通信。如下图5.11所示，龙芯3A3000处理器和龙芯7A1000桥片通过HyperTransport总线相连，7A1000桥片中集成GPU、DC（显示控制器）以及专供GPU和DC使用的显存控制器。CPU可以通过PIO方式读写GPU中的控制寄存器、DC中的控制寄存器以及显存；GPU和DC可以通过DMA方式读写内存，GPU和DC还可以读写显存。

<img src="计算机硬件结构.assets/图5.11 龙芯3A3000+7A1000两片方案.png" style="zoom:50%;" />

CPU或GPU周期性地把要显示的数据写入**帧缓存（Frame Buffer）**，DC根据帧缓存的内容进行显示。帧缓存可以分配在内存中，GPU和DC通过DMA方式访问内存中的帧缓存；而在独立显存的情况下，帧缓存分配在独立显存中，CPU直接把要显示的数据写入帧缓存，或者GPU通过DMA方式从内存中读取数据并把计算结果写入帧缓存，DC直接读取帧缓存的内容进行显示。

根据是否由GPU完成图形计算以及帧缓存是否分配在内存中，常见的显示模式有下图5.12所示四种。

<img src="计算机硬件结构.assets/图5.12 3A3000+7A1000的不同显示方式.png" style="zoom:50%;" />

模式一：不使用GPU，CPU与DC共享内存，如图5.12(a)所示。不使用桥片上的显存，而在内存中分配一个区域专供显示使用，这个区域称之为帧缓存。需要显示时，CPU通过正常内存访问将需要显示的内容写入内存中的帧缓存，然后通过PIO方式读写DC中的控制寄存器启动DMA，DC通过DMA操作读内存中的帧缓存并进行显示。

模式二：不使用GPU，DC使用独立显存，如图5.12(b)所示。DC使用桥片上的显存，这个区域称之为帧缓存。需要显示时，CPU将需要显示的内容从内存读出，再通过PIO方式写入独立显存上的帧缓存，然后通过PIO操作读写DC中的控制寄存器启动DMA，DC读显存上的帧缓存并进行显示。

模式三：CPU与GPU/DC共享内存，如图5.12(c)所示。需要显示时，CPU在内存中分配GPU使用的空间，并将相关数据填入，然后CPU通过PIO读写GPU中的控制寄存器启动DMA操作，GPU通过DMA读内存并将计算结果通过DMA写入内存中的帧缓存，CPU通过PIO方式读写DC中的控制寄存器启动DMA，DC通过DMA方式读内存中的帧缓存并完成显示。

模式四：GPU/DC使用独立显存，如图5.12(d)所示。需要显示时，CPU在内存中分配GPU使用的空间，并将相关数据填入，然后CPU通过PIO读写GPU中的控制寄存器启动DMA操作，GPU通过DMA读内存并将计算结果写入显存中的帧缓存，DC读显存中的帧缓存并完成显示。

# 六、计算机总线接口技术

一个完整的计算机系统包含多种组成部件，这些组成部件，一般不是由单个公司独立生产的，而是由不同的公司共同生产完成的，每个公司往往只能生产这些部件中的一种或者少数几种。为了让这些不同的部件组合在一起可以正常工作，必须制定一套大家共同遵守的规格和协议，这就是接口或者总线。按照中文的含义，接口是指两个对象连接的部分，而总线是指对象之间传输信息的通道。本文对接口和总线的概念不做语义上的区分，因为使用某种接口，必然需要使用与之相对应的总线；而总线也必然离不开接口，否则就无法使用。本文使用总线时，也包含与之相对应的接口；使用接口时，也包含与之相对应的总线。比如，提到USB时，既包含USB总线，也包含USB接口。

总线的应用和标准化，降低了计算机设计和应用的复杂度。有了标准化的接口，厂家生产出来的产品只需要接口符合规范，就可以直接与其他厂家生产的产品配合使用，而不必设计所有的硬件。总线的标准化，促进了计算机行业的分工合作，也极大地促进了计算机产业的发展。同时，标准化的总线也降低了计算机使用的成本，提高了用户使用的方便性。

总线技术涉及计算机的很多方面，除了物理链路外，还会涉及体系结构方面的内容。总线是不断演进发展的，目前应用的总线大都是在前代总线的基础上改进优化而来的，而且还在不断地改进。本章首先对总线的概念进行一个简单介绍，然后对当代计算机使用的总线进行简单分类，并按照一种分类原则分别介绍几种常用总线。

## （一）总线概述

总线的本质作用是完成数据交换。总线用于将两个或两个以上的部件连接起来，使得它们之间可以进行数据交换，或者说通信。总线含义很广，它不仅仅是指用于数据交换的通道，有时也包含了软件硬件架构。比如PCI总线、USB总线，它们不仅仅是指主板上的某个接口，还包含了与之相对应的整套硬件模型、软件架构。

总线的含义可以分为以下几个层级：

1. 机械层。接口的外形、尺寸、信号排列、连接线的长度范围等。
2. 电气层。信号描述、电源电压、电平标准、信号质量等。
3. 协议层。信号时序、握手规范、命令格式、出错处理等。
4. 架构层。硬件模型、软件架构等。

不同的总线包含的内容也有所不同，有的总线包含以上所有的层级，有的总线可能只包含部分层级。

## （二）总线分类

可以从多个角度对总线进行分类。

按照数据传递的方向，总线可以分为单向总线和双向总线。单向总线是指数据只能从一端传递到另一端，而不能反向传递。单向总线也称为单工总线。双向总线是指数据可以在两个方向上传递，既可以从A端传递到B端，也可以从B端传递到A端。双向总线也称为双工总线。双工总线又可分为半双工总线和全双工总线。半双工总线是指在一个时间段内，数据只能从一个方向传送到另一个方向，数据不能同时在两个方向传递。全双工总线是指数据可以同时在两个方向传递。全双工总线包含两组数据线，分别用于两个方向的数据传输。

按照总线使用的信号类型，总线可以分为并行总线和串行总线。并行总线包含多位传输线，在同一时刻可以传输多位数据，而串行总线只使用一位传输线，同一时刻只传输一位数据。并行总线的优点在于相同频率下总线的带宽更大，但正因为采用了同一时刻并行传输多位数据的方法，必须保证多位数据在同一时刻到达。这样就会对总线的宽度和频率产生限制，同时也对主板设计提出了更高的要求。与并行总线相反，一般串行总线只使用一位传输线，同一时刻只能传输一位数据，而且使用编码的方式将时钟频率信息编码在传输的数据之中，由此可以提升串行总线的传输频率。**PCI总线、DDR总线等都是传统的并行总线，而USB、SATA、PCIE等都是串行总线。以串行总线传输方式为基础，使用多条串行总线进行数据传输的方式正在被广泛采用。**以PCIE协议为例，PCIE的接口规范中，可以使用x1、x4、x8、x16等不同宽度的接口，其中，x16就是采用16对串行总线进行数据传输。多位串行总线与并行总线的根本差别在于，多位串行总线的每一个数据通道都是相对独立传输的，它们独立进行编解码，在接收端恢复数据之后再进行并行数据之间的对齐。而并行总线使用同一个时钟对所有的数据线进行同时采样，因此对数据传输线之间的对齐有非常严格的要求。

按照总线在计算机系统中所处的物理位置，总线可以分为片上总线、内存总线、系统总线和设备总线。下面将按照这个划分，分别举例介绍每种总线。

## （三）片上总线

片上总线是指芯片片内互连使用的总线。芯片在设计时，常常要分为多个功能模块，这些功能模块之间的连接即采用片上互连总线。例如，一个高性能通用处理器在设计时，常常会划分为处理器核、共享高速缓存、内存控制器等多个模块，而一个SoC片上系统芯片所包含的模块就更多了。下图6.1是一个嵌入式SoC芯片的内部结构，可以看到里面包含了很多功能模块，这些模块之间的连接就需要用到片上互连总线。

<img src="计算机硬件结构.assets/图6.1 龙芯2K1000芯片的结构图.png" style="zoom:50%;" />

这些模块形成了**IP（Intellectual Property）**，一家公司在设计芯片时常常需要集成其他公司的IP。这些IP的接口使用大家共同遵守的标准时，才能方便使用。因此，芯片的片上互连总线也形成了一定的标准。**目前业界公开的主流片上互连总线是ARM公司的AMBA系列总线。**

**高级微控制器总线架构（Advanced Microcontroller Bus Architecture，AMBA）**系列总线包括AXI、AHB、ASB、APB等总线。下面对AMBA总线的一些特点进行概括说明，这些总线的详细内容可以参阅相关总线协议。

### 1. AXI总线

**高级可扩展接口（Advanced eXtensible Interface，AXI）**总线是一种高性能、高带宽、低延迟的片上总线。它的地址/控制总线和数据总线是分离的，支持不对齐的数据传输，同时在突发传输中只需要发送首地址即可。它使用分离的读写数据通道并支持乱序访问。AXI是AMBA 3.0规范中引入的一个新的高性能协议，目标是满足超高性能和复杂的片上系统SoC的设计需求。

AXI总线主设备的主要信号定义如下表6.1所示。可以看到，AXI总线主要分为5个独立的通道，分别为写请求、写数据、写响应、读请求、读响应。每个通道采用握手协议独立传输。

<img src="计算机硬件结构.assets/表6.1 AXI总线主要信号定义.png" style="zoom:50%;" />

AXI协议包括以下特点：

- 单向通道体系结构。信息流只以单方向传输，符合片内设计的要求。
- 支持多项数据交换。AXI协议支持的数据宽度很宽，最大可到1024位。通过并行执行突发（Burst）操作，极大地提高了数据吞吐能力，可在更短的时间内完成任务。
- 独立的地址和数据通道。地址和数据通道分开便于对每一个通道进行单独优化，可以根据需要很容易地插入流水级，有利于实现高时钟频率。

(1) AXI架构

AXI协议是一个主从协议，每套总线的主设备和从设备是固定好的，只有主设备才可以发起读写命令。一套主从总线包含五个通道：写地址（请求）通道、写数据通道、写响应通道、读地址（请求）通道、读返回（响应）通道。读/写地址通道用来传送读写目标地址、数据宽度、传输长度和其他控制信息。写数据通道用来由主设备向从设备传送写数据，AXI支持带掩码的写操作，可以指定有效数据所在的字节。写响应通道用来传送写完成信息。读返回通道用来传送从设备读出的数据以及响应信息。

AXI协议的一次完整读写过程称为一个总线事务（Transaction），传输一个周期的数据称为一次传输（Transfer）。AXI协议允许地址控制信息在数据传输之前发生，并且支持多个并发访问同时进行，它还允许读写事务的乱序完成。下图6.2和图6.3分别说明了读写事务是如何通过读写通道进行的。

<img src="计算机硬件结构.assets/图6.2;图6.3 写事务架构与读事务架构.png" style="zoom:50%;" />

AXI使用双向握手协议，每次传输都需要主从双方给出确认信号。数据的来源方设置有效（Valid）信号，数据的接收方设置准备好（Ready）信号。只有当有效信号和准备好信号同时有效时，数据才会传输。读请求通道和写数据通道还各包含一个结束（Last）信号来指示一次突发传输的最后一个传输周期。

(2) 互连架构

在一个使用AXI总线的处理器系统中，一般都会包含多个主设备和从设备。这些设备之间使用互连总线进行连接，如下图6.4所示。在该互连结构中，任意一个主设备都可以访问所有的从设备。比如，主设备2可以访问从设备1、2、3、4。

<img src="计算机硬件结构.assets/图6.4 AXI设备的接口和互连.png" style="zoom:50%;" />

为了减少互连结构的信号线个数，AXI的互连结构可以共享地址和数据通道，或者共享地址通道但使用多个数据通道。当需要连接的主从设备个数较多时，为了减少互连结构的信号线个数，AXI协议还可以很方便地支持多层次的互连结构。

(3) 高频设计

AXI协议的每个传输通道都只是单向的信息传递，并且AXI协议对多个通道之间的数据传输没有规定特定的顺序关系，多个通道之间没有同步关系。因此，设计者可以很容易地在通道中插入寄存器缓冲，这对于高频设计是很重要的。

(4) 基本事务

下面简要介绍AXI的读写事务。AXI协议的主要特点是使用VALID和READY握手机制进行传输。地址和数据信息都只在VALID和READY信号同时为高有效的情况下才进行传输。

下图6.5显示了一个突发读事务的传输，其中请求由主设备发往从设备，响应由从设备发往主设备。

<img src="计算机硬件结构.assets/图6.5 突发读事务.png" style="zoom:50%;" />

地址信息在T2传输后，主设备从T4时刻开始给出读数据READY信号，从设备保持读数据VALID信号为低，直到读数据准备好后，才在T6时刻将读数据VALID信号拉高，主设备在T5时刻接收读数据。当所有读数据传输完成后，在T13时刻，从设备将RLAST信号拉高表示该周期是最后一个数据传输。

下图6.6显示了一个重叠的读事务。

<img src="计算机硬件结构.assets/图6.6 重叠的读事务.png" style="zoom:50%;" />

在T4时刻，事务A的读数据还没有完成传输，从设备就已经接收了读事务B的地址信息。重叠事务使得从设备可以在前面的数据没有传输完成时就开始处理后面的事务，从而降低后面事务的完成时间。AXI总线上，通过ID对不同的事务加以区别。同一个读事务的请求与响应中，ARID与RID相同；同一个写事务的请求与响应中，AWID、WID与BID相同。

下图6.7是一个写事务的示例。

<img src="计算机硬件结构.assets/图6.7 写事务.png" style="zoom:50%;" />

主从设备在T2时刻传输写地址信息，接着主设备将写数据发送给从设备，在T9时刻，所有的写数据传输完毕，从设备在T10时刻给出写完成响应信号。

(5) 读写事务顺序

AXI协议支持读写事务乱序完成。每一个读写事务都有一个ID标签，该标签通过AXI信号的ID域进行传输。同ID的读事务或者同ID的写事务必须按照接收的顺序按序完成，不同ID的事务可以乱序完成。如图6.6的例子，图中事务A的请求发生在事务B的请求之前，从设备响应时事务A的数据同样发生在事务B的数据之前，这就是顺序完成。如果事务A与事务B使用了不同的ID，那么从设备就可以先返回事务B的数据再返回事务A的数据。

(6) AXI协议的其他特点

AXI协议使用分离的读写地址通道，读事务和写事务都包含一个独立的地址通道，用来传输地址和其他控制信息。

AXI协议支持下列传输机制：

- 不同的突发传输类型。AXI支持回绕（Wrapping）、顺序（Incrementing）和固定（Fix）三种传输方式。回绕传输适合高速缓存行传输，顺序传输适合较长的内存访问，固定传输则适合对外设FIFO的访问。
- 传输长度可变。AXI协议支持1到16甚至更多个传输周期。
- 传输数据宽度可变。支持8\~1024位数据宽度。
- 支持原子操作。
- 支持安全和特权访问。
- 支持错误报告。
- 支持不对齐访问。

### 2. AHB、ASB、APB总线

AHB、ASB、APB总线是在AXI总线之前推出的系统总线，本书只对它们进行简要总结，详细内容可参阅相关协议文档。

**AHB（Advanced High-performance Bus）**总线是高性能系统总线，它的读写操作共用命令和响应通道，具有突发传输、事务分割、流水线操作、单周期总线主设备切换、非三态实现以及宽数据总线等特点。AHB协议允许8\~1024位的数据总线宽度，但推荐的数据宽度最小为32位，最大为256位。

**ASB（Advanced System Bus）**是第一代AMBA系统总线，同AHB相比，它支持的数据宽度要小一些，典型数据宽度为8位、16位、32位。它的主要特征有，流水线方式，数据突发传送，多总线主设备，内部有三态实现。

**APB（Advanced Peripheral Bus）**是本地二级总线（Local Secondary Bus），通过桥和AHB/ASB相连。它主要是为了满足不需要高性能流水线接口或不需要高带宽接口的设备间的互连。其主要优点是接口简单、易实现。

基于AMBA总线的计算机系统的结构如下图6.8和图6.9所示。

<img src="计算机硬件结构.assets/图6.8 使用AHB和APB连接的微控制器系统.png" style="zoom:50%;" />

<img src="计算机硬件结构.assets/图6.9 使用AXI总线互连的通用高性能处理器.png" style="zoom:50%;" />

片上互连总线的最大特点是高并行性。由于片内走线的距离短，线宽细，因此可以实现高并行性。片上互连总线的设计需要考虑总线的通用性、可扩展性、性能以及总线接口逻辑的设计简单性等方面。

## （四）内存总线

内存总线用于连接处理器和主存储器。目前使用的主存储器是DRAM芯片，而内存控制器和内存芯片（或者说内存条）的接口就是内存总线。内存总线规范是由*JEDEC（Joint Electron Device Engineering Council）*组织制定的，它包含了一般总线的三个层级：机械层、电气层、协议层。

在机械层，JEDEC规定了内存芯片的封装方式、封装大小和引脚排布，内存条生产厂家可以据此设计内存条的**印刷电路板（Printed Circuit Board，PCB）**，可以使用不同DRAM厂家的芯片。同时，JEDEC也制定了内存条和计算机主板连接的规范，也就是内存插槽规范，规定了内存条的引脚个数、排布和内存条的长度、厚度、机械形式。这样不同厂家的内存条就可以在同一块主板上使用。

在电气层，JEDEC组织规定了DRAM芯片的电气特性。例如，DDR2内存使用1.8V电压，而DDR3内存使用1.5V电压。另外，规范还规定输入电压高低电平的标准、信号斜率、时钟抖动的范围等信号电气特性。

在协议层，JEDEC组织规定了DRAM芯片的操作时序。协议规定了DRAM芯片的上电和初始化过程、DRAM工作的几种状态、状态之间的转换，以及低功耗控制等内容。比如，DRAM初始化完成后，进入空闲态，通过激活（Activate）命令进入“打开一行”的激活态，只有在激活态，才可以读写DRAM的数据，单纯的读写操作后，DRAM仍会处于激活态，等待下一次读写。如果想要读写其他行，需要首先发送预充（Precharge）命令将DRAM转回空闲态，然后再发送激活命令。这些命令不是在任意时刻都可以发送的，需要满足协议规定的时序要求。下图6.12给出了DDR2内存的状态转换图。

<img src="计算机硬件结构.assets/图6.12 DDR2内存各状态转换图.png" style="zoom:50%;" />

DDR3内存条的接口信号见下表6.2。内存条将多个DDR3 SDRAM存储芯片简单地并列在一起，因此表中所列的信号主要是DDR3 SDRAM的信号。

<img src="计算机硬件结构.assets/表6.2 双面DDR3 UDIMM内存条的接口信号列表.png" style="zoom:50%;" />

此外，表中还包含了一组I2C总线信号（SCL、SDA）和I2C地址信号（SA0\~SA2）用来支持内存条的软件识别。内存条将自身的一些设计信息（包括SDRAM类型、SDRAM的速度等级、数据宽度、容量以及机械尺寸标准等信息）保存在一个EEPROM中，该EEPROM可以通过I2C总线访问，称为SPD（Serial Present Detect）。计算机系统可以通过I2C总线来读取内存条的信息，从而自动匹配合适的控制参数并获取正确的系统内存容量。组装电脑时，用户可以选用不同容量、品牌的内存条而无须修改软件或主板，就离不开SPD的作用。值得一提的是，表中给出的信号是按照双面内存条带ECC功能列出来的，如果只有单面，或者不带ECC校验功能，只需将相应的引脚位置悬空。

DRAM存储单元是按照Bank、行、列来组织的，因此对DRAM的寻址是通过Bank地址、行地址、列地址来进行的。此外，计算机系统中可以将多组DRAM串接在一起，不同组之间通过片选（CS）信号来区分。在计算机系统中，程序的地址空间是线性的，处理器发出的内存访问地址也是线性的，由内存控制器负责将地址转换为对应于DRAM的片选、Bank地址、行地址、列地址。

DDR3 SDRAM读操作时序如下图6.13所示。图中的命令（Command，CMD）由RAS_n、CAS_n和WE_n三个信号组成。当RAS_n为高电平，CAS_n为低电平，WE_n为高电平时，表示一个读命令。

<img src="计算机硬件结构.assets/图6.13 DDR3 SDRAM读时序.png" style="zoom:50%;" />

该图中，列地址信号延迟（CL）等于5个时钟周期，读延迟（RL）等于5个时钟周期，突发长度（Burst Length，BL）等于8个时钟周期。控制器发出读命令后，经过5个时钟周期，SDRAM开始驱动DQS和DQ总线输出数据。DQ数据信号和DQS信号是边沿对齐的。在DQS的起始、DQ传输数据之前，DQS信号会有一个时钟周期长度的低电平，称为读前导（Read Preamble）。读前导的作用是给内存控制器提供一个缓冲时间，以开启一个信号采样窗口，将有用的读数据采集到内部寄存器，同时又不会采集到数据线上的噪声数据。

DDR3 SDRAM写操作的协议如下图6.14所示。当RAS_n为高电平，CAS_n为低电平，WE_n为低电平时，表示一个写操作。读写操作命令的区别是WE_n信号的电平不同，读操作时该信号为高，写操作时该信号为低。

<img src="计算机硬件结构.assets/图6.14 DDR3 SDRAM写时序.png" style="zoom:50%;" />

写操作使用额外的数据掩码（Data Mask，DM）信号来标识数据是否有效。当DM为高时，对应时钟沿的数据并不写入SDRAM，当DM为低时，对应时钟沿的数据才写入SDRAM。DM信号与DQ信号同步。在写操作时，DQS信号和DQ信号是由内存控制器驱动的。同样，在DQS的起始、DQ传输数据之前，DQS信号也存在一个写前导（Write Preamble）。DDR3 SDRAM的写前导为一个周期的时钟信号，而DDR2 SDRAM的写前导为半个时钟周期的低电平信号。

前述SDRAM的基本操作包括激活（Activate）、读写（Read/Write）和预充电（Precharge）。当SDRAM接收到一个操作后，它需要在固定的时钟周期之后开始进行相应的动作，并且这些动作是需要经过一定的时间才能完成的。因此，对DRAM不同操作命令之间是有时间限制的。例如，对于DDR3-1600内存来说，当软件访问的两个地址正好位于内存的同一个Bank的不同行时，内存控制器需要首先针对第一个访问地址发出激活操作，经过13.75ns的时间，才可以发出第一次的读写操作。如果第一个访问是读操作，则需要经过至少7.5ns（此外还需满足tRASmin的要求，这里进行简化说明）的时间才可以发送下一次的预充电操作。预充电操作发送后，需要经过13.75ns的时间才可以针对第二个访问的行地址发送新的激活操作，然后经过13.75ns的时间，发送第二次读写操作。上述访问过程如下图6.15所示。

<img src="计算机硬件结构.assets/图6.15 SDRAM的访问时序图.png" style="zoom:50%;" />

可以发现，对SDRAM的同一个Bank的不同行进行读写存在较大的访问延迟。为了掩盖访问延迟，SDRAM允许针对不同Bank的操作并发执行。

提高内存总线访问效率的两个主要手段是充分利用行缓冲局部性和Bank级并行度。行缓冲局部性是说，当两个访存命令命中SDRAM的同一行时，两个命令可以连续快速发送；Bank级并行度是说，针对SDRAM的不同Bank的操作可以并发执行，从而降低后一个操作的访存延迟。

下面以一个简单的例子来说明对SDRAM的不同访问序列的延迟的差别。假定使用的是DDR2-800E规格的内存，它对应的时序参数为：tRCD=15ns，tRP=15ns，tRASmin=45ns，tRC=60ns，tRL=15ns，tRTP=7.5ns，tCCD=10ns（4个时钟周期）。处理器发出了三个访存读命令，地址分别命中SDRAM的第0个Bank的第0行（列地址为0）、第0个Bank的第1行和第0个Bank的第0行（与第一个命令的列地址不同，假定列地址为1）。如果我们不改变访问的顺序，直接将这三个命令转换为对应SDRAM的操作发送给内存，则需要的时间如下图6.16所示。

<img src="计算机硬件结构.assets/图6.16 调度前的命令序列.png" style="zoom:50%;" />

图6.16中，(B0,R0)表示第0个Bank的第0行，(B0,R1)表示第0个Bank的第1行。每一个读命令都会转换出对应于SDRAM的“激活，读数据，预充电”序列。则读数据分别在第30ns（tRCD+tRL）、90ns（tRC+tRCD+tRL）、150ns（tRC+tRC+tRCD+tRL）开始返回给处理器。

假定我们改变命令发给内存的顺序，我们将第3个命令放到第1个命令之后发送，将第2个命令最后发送，则得到的访存序列如下图6.17所示。

<img src="计算机硬件结构.assets/图6.17 调度后的命令序列.png" style="zoom:50%;" />

在该图6.17中，针对第0个Bank第0行第1列的命令不需要发送预充电和激活操作，而是在针对第0个Bank第0行第0列的命令之后直接发送。则处理器得到读数据的时间变为第30ns、第40ns、第90ns。相比上一种访存序列，第3个访存命令的读数据的访存延迟降低了110ns（40ns相比于150ns）。

对内存总线的控制是由内存控制器实现的。内存控制器负责管理内存条的初始化、读写、低功耗控制等操作。内存控制器接收处理器发出的读写命令，将其转化为内存芯片可以识别的DRAM操作，并负责处理时序相关问题，最终返回数据（对于读命令）或者返回一个响应（对于写命令）给处理器。内存控制器一般还包括命令调度功能，以提高内存总线的访问效率。对于处理器来说，它只需要发送读写命令给内存控制器就可以了，而不必关心内存的状态以及内存是如何被读写的。

## （五）系统总线

系统总线通常用于处理器与桥片的连接，同时也作为多处理器间的连接以构成多路系统。英特尔处理器所广泛采用的**QPI（Quick Path Interconnect）**接口及在QPI之前的**FSB（Front Side Bus）**，还有AMD处理器所广泛采用的**HT（HyperTransport）**接口都属于系统总线。

系统总线是处理器与其他芯片进行数据交换的主要通道，系统总线的数据传输能力对计算机整体性能影响很大。如果没有足够带宽的系统总线，计算机系统的外设访问速度会明显受限，类似于显示、存储、网络等设备的交互都会受到影响。随着计算机系统技术的不断进步，微处理器与其他芯片间的数据传输性能成为制约系统性能进一步提升的一个重要因素。为了提升片间传输性能，**系统总线渐渐由并行总线发展为高速串行总线**。

下面以HyperTransport总线为例介绍系统总线。

### 1. HyperTransport总线

HyperTransport总线（简称HT总线）是AMD公司提出的一种高速系统总线，用于连接微处理器与配套桥片，以及多个处理器之间的互连。HT总线提出后，先后发展了HT1.0、HT2.0、HT3.0等几代标准，目前最新的标准为HT3.1。

下图6.18是采用HT总线连接处理器与桥片的结构示意图。

<img src="计算机硬件结构.assets/图6.18 CPU-南桥两片方案.png" style="zoom:50%;" />

与并行总线不同的是，串行总线通常采用点对点传输形式，体现在计算机体系结构上，就是一组串行总线只能连接两个芯片。以龙芯3A2000/3A3000为例，在四路互连系统中，一共采用了7组HT互连总线，其中6组用于四个处理器间的全相联连接，1组用于处理器与桥片的连接，如下图6.19所示。

<img src="计算机硬件结构.assets/图6.19 龙芯3A2000、3A300四路系统结构示意图.png" style="zoom:50%;" />

而作为对比，PCI总线则可以在同一组信号上连接多个不同的设备，如下图6.20所示。

<img src="计算机硬件结构.assets/图6.20 PCI总线设备连接.png" style="zoom:50%;" />

HT总线的软件架构与PCI总线协议基本相同，都采用配置空间、IO空间和Memory空间的划分，通过对配置寄存器的设置，采用正向译码的方向对设备空间进行访问。基于PCI总线设计的设备驱动程序能够直接使用在HT总线的设备上。

但在电气特性及信号定义上，HT总线与PCI总线却大相径庭，HT由两组定义类似但方向不同的信号组成，其主要信号定义如下表6.3所示。HT总线主要包括三种信号，分别为CLK、CTL、CAD，其中CLK作为随路时钟使用，用于传递总线的频率信息并用作数据恢复，CAD[n∶0]通常是以8位为单位，共用一组时钟信号，总线宽度可以为8位、16位或32位。

<img src="计算机硬件结构.assets/表6.3 HT总线主要信号定义.png" style="zoom:50%;" />

下图6.21中两个芯片通过定义相同的信号进行相互传输。

<img src="计算机硬件结构.assets/图6.21 HT总线连接.png" style="zoom:50%;" />

与上一节介绍的DDR内存总线所不同的是，HT总线上，用于数据传输的信号并非双向信号，而是由两组方向相反的单向信号各自传输。这种传输方式即通常所说的全双工传输。发送和接收两个方向的传输可以同时进行，互不干扰。而采用双向信号的总线，例如DDR内存总线或者PCI总线，只能进行半双工传输，其发送和接收不能同时进行。而且在较高频率下，发送和接收两种模式需要进行切换时，为了保证其数据传输的完整性，还需要在切换过程中增加专门的空闲周期，这样更加影响了总线传输效率。

PCI接口信号定义如下图6.22所示。PCI总线上使用起始信号（FRAME\#）及相应的准备好信号（TRDY\#、IRDY\#）、停止信号（STOP\#）来进行总线的握手，控制总线传输。

<img src="计算机硬件结构.assets/图6.22 PCI总线信号定义.png" style="zoom:50%;" />

与PCI总线不同，HT总线信号定义看起来非常简单，没有类似PCI总线的握手信号。实际上HT总线的读写请求是通过包的形式传输的，将预先定义好的读写包通过几个连续的时钟周期进行发送，再由接收端进行解析处理。同时，HT总线采用了流控机制替代了握手机制。

流控机制的原理并不复杂。简单来说，在总线初始化完成后，总线双方的发送端，将自身的接收端能够接收的请求或响应的数目，通过一种专用的流控包通知对方。总线双方各自维护一组计数器记录这个数目信息。每需要发出请求或响应时，先检查对应的计数器是否为0，如果为0，表示另一方无法再接收这种请求或响应，发送方需要等待；如果不为0，则将对应的计数器值减1，再发出请求或响应。而接收端每处理完一个请求或响应后，会再通过流控包通知对方，对方根据这个信息来增加内部对应的计数器。正是通过这种方式，有效消除了总线上的握手，提升了总线传输的频率和效率。

这种传输模式对提升总线频率很有好处。PCI总线发展到PCI-X时，频率能够达到133MHz，宽度最高为64位，总线峰值带宽为1064MB/s。而HT总线发展到3.1版本时，频率能够达到3.2GHz，使用双沿传输，数据速率达到6.4Gb/s，以常见的16位总线来说，单向峰值带宽为12.8GB/s，双向峰值带宽为25.6GB/s。即使去除地址命令传输周期，其有效带宽也比PCI总线提升了一个数量级以上。

### 2. HT包格式

HT总线的传输以包为单位。按照传输的类型，首先分为控制包和数据包两种。控制包和数据包使用CTL信号区分，当CTL信号为高有效时，表示正在传输一个控制包，当CTL信号为低时，表示正在传输一个数据包。数据包依附于最近的一个带数据的控制包。控制包根据传输的内容，再分为三种不同的包格式，分别为信息包、请求包、响应包。

信息包的作用是为互连的两端传递底层信息，本身不需要流控。这意味着对于信息包，无论何时都是可以被接收并处理的。流控信息就是一种典型的信息包。信息包的格式如下表6.4所示。

<img src="计算机硬件结构.assets/表6.4 HT信息包格式.png" style="zoom:50%;" />

其中，“命令”域用于区分不同的包，对不同的命令，包的其他位置表示的内容之间有所不同。

HT也是采用**双倍数据率（Double Data Rate，DDR）传输，即在时钟的上升、下降沿各传一组数据**。每种包大小都是4字节的倍数。下图6.23是在总线上传输的时序示意图，以8位的CAD总线为例。在CTL为高电平的时候，表示传输的是控制包，而CTL为低时，表示传输的是数据包。图中CAD信息上的数字对应包格式表中的具体拍数。

<img src="计算机硬件结构.assets/图6.23 HT总线传输示意图.png" style="zoom:50%;" />

下表6.5为请求包的格式。因为需要传输地址信息，请求包最少需要8字节；当使用64位地址时，请求包可以扩展至12字节。大部分请求包地址的[7:2]是存放在第3拍。因为数据的最小单位为4字节，地址的[1:0]不需要进行传输。当传输的数据少于4字节时，利用数据包的屏蔽位进行处理。

<img src="计算机硬件结构.assets/表6.5 HT请求包格式.png" style="zoom:50%;" />

请求包主要是读请求和写请求。其中读请求不需要数据，而写请求需要跟随数据包。

下表6.6是响应包的格式。响应包大小为4字节。与请求包类似，写响应包不需要数据，而读响应包需要跟随数据包。

<img src="计算机硬件结构.assets/表6.6 HT相应包格式.png" style="zoom:50%;" />

下面表6.7和表6.8分别是数据包的两种格式。在请求包或响应包中定义了专门的数据长度信息，这个长度以4字节为基本单位，最长为16，也就是64字节。因此数据包的大小是4字节的整数倍，最大为64字节。

表6.7为不带屏蔽位的数据包格式，最长可以为64字节。

<img src="计算机硬件结构.assets/表6.7 HT无屏蔽位数据包格式.png" style="zoom:50%;" />

表6.8为带屏蔽位的数据包格式，前4个字节用于定义数据的使能/屏蔽信息，每一位对应一个字节，最多可以为32个字节。也就是说，对于带屏蔽位的数据，一个数据包最多传输32字节数据。对于数据包，长度最多就是$32+4=36$个字节。

<img src="计算机硬件结构.assets/表6.8 HT带屏蔽位数据包格式.png" style="zoom:50%;" />

## （六）设备总线

设备总线用于计算机系统中与IO设备的连接。**PCI（Peripheral Component Interconnect）**总线是一种对计算机体系结构连接影响深远并广泛应用的设备总线。**PCIE（PCI Express）**可以被看作PCI总线的升级版本，兼容PCI软件架构。PCIE总线被广泛地用作连接设备的通用总线，在现有计算机系统中已经基本取代了PCI的位置。PCIE接口在系统中的位置如下图6.24所示，一般与SATA、USB、显示等设备接口位于同样层次，用于扩展外部设备。

<img src="计算机硬件结构.assets/图6.24 PCIE接口位置示意图.png" style="zoom:50%;" />

### 1. PCIE总线

与HT类似，PCIE总线也是串行总线。PCIE与设备进行连接的时候同样采用点对点的方式，一组PCIE接口只能连接一个设备。为了连接多个设备，就需要实现多个接口，如下图6.25所示。

<img src="计算机硬件结构.assets/图6.25 PCIE接口连接示意图.png" style="zoom:50%;" />

与HT又有所不同，两者在信号定义和接收发送方法上有很大差别。PCIE的总线信号如下表6.9所示。

<img src="计算机硬件结构.assets/表6.9 PCIE总线主要信号定义.png" style="zoom:50%;" />

可以看到，PCIE接口上只有用于数据传输的信号。PCIE接口上的各个TX信号之间相互独立，最小单位为1位，称之为通道（Lane），这些通道是一些高速的串行链路，这些链路组合在一起构成了X1、X2、X4、X8或X16链路，对应常见的1位、4位、8位及16位总线宽度。**每个通道（或链路）是全双工的**，即能够同时读写，所以**一个数据链路使用2个物理引脚（RX和TX）**。在**提到PCIE链路带宽时通常是指单向带宽**，即1个读引脚或1个写引脚的带宽，例如1Gb/s的链路带宽，对应于1Gb/s的读带宽和1Gb/s的写带宽。对于全双工链路来说，其**吞吐量**是读带宽与写带宽之和，而半双工链路，吞吐量只是单向带宽。

如千兆网卡、SATA扩展卡、USB扩展卡等总线宽度大多为1位，而显卡、RAID卡等总线宽度通常为16位。

PCIE在进行传输时，仅仅发送数据信号，而没有发送时钟信号。在接收端通过总线初始化时约定好的数据序列恢复出与发送端同步的时钟，并使用该时钟对接收到的数据信号进行采样，得到原始数据。

### 2. PCIE包格式

PCIE总线的传输同样以**（事务层）包（Transaction Level Packet，TLP）**为单位，其包格式如下图6.26所示。PCIE包主要分为TLP首部与数据负载两部分，其作用与HT包类似，可以对应到HT包中的控制包与数据包。PCIE包同样是以4字节为单位增长。

<img src="计算机硬件结构.assets/图6.26 PCIE总线包格式.png" style="zoom:50%;" />

对于具体包格式的定义，PCIE与HT各有不同。尤其是PCIE包在协议上最多可以一次传输4KB的数据，而HT包最多一次传输64字节。PCIE的具体包格式定义在此不再展开，感兴趣的读者可以参考PCIE相关协议。此外，PCIE同样采用了流控机制来消除总线握手。

PCIE总线被广泛地用作连接设备的设备总线，而HT总线则作为系统总线，用于处理器与桥片之间的连接及多处理器间的互连。

这些使用上的差异是由总线接口特性所决定的。与HT总线不同的是，PCIE接口在x1时，只有一对发送信号线和一对接收信号线，没有随之发送的时钟和控制信号。PCIE接口通过总线传输编码，将时钟信息从总线上重新提取并恢复数据。PCIE总线的传输相比HT总线来说，开销更大，带来延迟的增大及总线带宽利用率的降低。

PCIE总线可以由多个数据通道组成，每个通道只有一对发送信号和一对接收信号，因此传输时每个通道所使用的信号线更少，而且不同的通道之间相关性小，目前使用的PCIE卡最多为16个数据通道。对于物理连接来说，PCIE接口相比HT接口，实现更为简单，被广泛地用作可扩展设备连接，逐渐替代了PCI总线。

# 七、计算机系统启动过程分析

前面章节主要从计算机硬件的角度对构成计算机系统的各个主要部分进行了介绍。为了描述计算机硬件系统各部分之间的相互关系，本章将对计算机从开机到点亮屏幕，接收键盘输入，再到引导并启动整个操作系统的具体过程进行探讨。与本书其他章节一样，本章基于LoongArch架构进行介绍，具体则以龙芯3号处理器的启动过程为例。

无论采用何种指令系统的处理器，复位后的第一条指令都会从一个预先定义的特定地址取回，处理器的执行就从这条指令开始。处理器的启动过程，实际上就是一个特定程序的执行过程。这个程序我们称之为固件，又称为BIOS（Basic Input Output System，基本输入输出系统）。对于LoongArch，处理器复位后的第一条指令将固定从地址0x1C000000的位置获取。**这个地址需要对应一个能够给处理器核提供指令的设备，这个设备以前是各种ROM，现在通常是闪存（Flash）。**从获取第一条指令开始，计算机系统的启动过程也就开始了。

为了使计算机达到一个最终可控和可用的状态，在启动过程中，需要对包括处理器核、内存、外设等在内的各个部分分别进行初始化，再对必要的外设进行驱动管理。本章的后续内容将对这些具体工作进行讨论。

## （一）处理器核初始化

所谓初始化，实际上是将计算机内部的各种寄存器状态从不确定设置为确定，将一些模块状态从无序强制为有序的过程。简单来说，就是通过load/store指令或其他方法将指定寄存器或结构设置为特定数值。

举例来说，在MIPS和LoongArch结构中，都只将0号寄存器的值强制规定为0，而其他的通用寄存器值是没有要求的。在处理器复位后开始运行的时候，这些寄存器的值可能是任意值。如果需要用到寄存器内容，就需要先对其进行赋值，将这个寄存器的内容设置为软件期望的值。这个赋值操作可以是加载立即数，也可以是对内存或者其他特定地址进行load操作，又可以是以其他已初始化的寄存器作为源操作数进行运算得到的结果。

这个过程相对来说比较容易理解，因为是对软件上需要用到的单元进行初始化。而另一种情况看起来就相对隐蔽一些。例如，在现代处理器支持的猜测执行、预取等微结构特性中，可能会利用某些通用寄存器的值或者高速缓存的内容进行猜测。如果整个处理器的状态并没有完全可控，或许会猜测出一些极不合理的值，导致处理器微结构上执行出错而引发死机。这样就需要对一些必要的单元结构进行初始化，防止这种情况发生。

举一个简单的例子。计算机系统中使用约定的应用程序二进制接口（Application Binary Interface，ABI）作为软件接口规范。LoongArch约定使用1号寄存器（\$r1）作为函数返回指针寄存器（ra，Return Address），函数返回时，一般使用无条件跳转指令“jirl”，这条指令的格式为`jirl rd, rj, offset`，其中rj与offset表示跳转的目标地址，同时将当前PC值加4写道通用寄存器rd中，当不需要保存时，可以指定为\$r0，也就是0号寄存器。因此，函数返回时，一般可用`jirl $r0, $r1, 0`来实现。这样，一种可行的转移预测优化方法是在指令译码得到“jirl”指令时，立即使用\$r1作为跳转地址进行猜测取指，以加速后续的指令执行。如果程序中没有使用`jirl $r0, $r1, 0`，而是采用了诸如`jirl $r0, $r2, 0`这样的指令，就会导致这个猜测机制出错。而如果此时\$r1的寄存器是一个随机值，就有可能导致取指猜测错误，发出一个对非法地址的猜测请求。所以如果此时处理器没有对猜测访问通路进行控制或初始化，就可能会发生严重问题，例如猜测访问落入地址空洞而失去响应并导致死机等。

为了防止这个问题，在处理器开始执行之后，一方面需要先对相关的寄存器内容进行初始化，设置为一个正常地址值，另一方面则需要对地址空间进行处理，防止出现一般情况下不可访问的地址空洞。这样即使发生了这种预期外的猜测访问，也可以得到响应，避免系统出错或死机。

整个处理器由系统复位到操作系统启动的简要流程如下图7.1所示。其中第一列为处理器核初始化过程，第二列为芯片核外部分初始化过程，第三列为设备初始化过程，第四列为内核加载过程，第五列为多核芯片中的从核（Slave Core）独有的启动过程。

<img src="计算机硬件结构.assets/图7.1 系统复位到操作系统启动的简要流程图.png" style="zoom:50%;" />

### 1. 处理器复位

处理器的第一条指令实际上是由复位信号控制的，但受限于各种其他因素，复位信号并没有对处理器内部的所有部分进行控制，例如TLB、Cache等复杂结构，而是只保证从取指部件到BIOS的取值通路畅通。如果把CPU比作一个大房间，复位后的房间内部漆黑一片，大门（内存接口）、窗户（IO接口）都是关着的，只有微弱的灯光照亮了通向一扇小门（BIOS接口）的通路。

在LoongArch架构下，处理器复位后工作在直接地址翻译模式下，该模式下的地址为虚实地址直接对应（相等）的关系，也就是不经TLB映射，也不经窗口映射。默认情况下，无论是取指访问还是数据访问，都是Uncache模式，也即不经缓存。这样即使硬件不对TLB、Cache两个结构进行初始化，处理器也能正常启动并通过软件在后续的执行中对这些结构进行初始化。尤其是早期的处理器设计由于对资源或时序的考虑，出于简化硬件设计的目标，将很多初始化工作交由软件进行。但**现在大部分处理器在硬件上自动处理初始化**，从而减轻软件负担，缩短系统启动时间。例如，龙芯3A1000和龙芯3B1500都没有实现硬件初始化功能，只能通过软件对Cache进行初始化。本身Cache的初始化就需要运行在Uncache的空间上，执行效率低下，而且当Cache越来越大时，所需要的执行时间就越来越长。从龙芯3A2000开始，龙芯处理器也实现了TLB、各级Cache等结构的硬件初始化。**硬件初始化的时机是在系统复位解除之后、取指访问开始之前，以此来缩短BIOS的启动时间。**

LoongArch处理器复位后的第一条指令将固定从地址0x1C000000的位置获取，这个过程是由处理器的执行指针寄存器被硬件复位为0x1C000000而决定的。对物理地址0x1C000000的取指请求，会被处理器内部预先设定好的片上互连网络路由至某个预先存放着启动程序的存储设备。从第一条指令开始，处理器核会依据软件的设计按序执行。

以龙芯3A5000处理器为例，处理器得到的前几条指令通常如下。上面为手工编写的代码，下面为编译器编译生成的汇编代码。其中的stack、\_gp为在代码其他地址所定义的标号，编译器编译时能够使用实际的地址对其进行替换。其中lu12i.w用于将20位立即数符号扩展并装载到寄存器的比特[63:12]，lu32i.d用于将20位立即数符号扩展并装载到寄存器的比特[63:32]，lu52i.d用于将12位立即数装载到寄存器的比特[63:52]，ori用于将12位立即数与寄存器的内容进行或操作。

```assembly
dli     t0, (0x7<<16)
csrxchg zero, t0, 0x4
dli     t0, 0x1c001000
csrwr   t0, 0xc
dli     t0, 0x1c001000
csrwr   t0, 0x88
dli     t0, (1<<2)
csrxchg zero, t0, 0x0
la      sp, stack
la      gp, _gp
```

```assembly
lu12i.w  $r12, 0x70
csrxchg  $r0, $r12, 0x4
lu12i.w  $r12, 0x1c001
csrwr    $r12, 0xc
lui12i.w $r12, 0x1c001
csrwr    $r12, 0x88
ori      $r12, $r0, 0x4
csrxchg  $r0, $r12, 0x0
lu12i.w  $r3, 0x90400
lu32i.d  $r3, 0
lu52i.d  $r3, $r3, 0x900
lu12i.w  $r2, 0x90020
ori      $r2, $r2, 0x900
lu32i.d  $r2, 0
lu52i.d  $r2, $r2, 0x900
```

这几条指令对处理器核的中断处理相关寄存器进行了初始化，并对后续软件将使用的栈地址等进行了初始化。第一条csrxchg指令将例外配置寄存器（0x4偏移）中的比特[18:16]设置为0，以将除TLB外的所有例外和中断入口设置为同一个（代码中的0x1C001000）。第一条csrwr指令将该例外入口地址（0xC号控制寄存器）设置为0x1C001000，第二条csrwr指令将TLB重填例外的入口地址（0x88号控制寄存器）也设置为0x1C001000。实际上BIOS并没有使用TLB地址映射，一旦出现了TLB重填例外，一定是使用的地址出现了错误。第二条csrxchg指令将模式信息寄存器（0x0号控制寄存器）中的比特[2]设置为0，以禁用所有的中断。可以看到，对于stack、gp这些地址的装载所用的la指令，在经过编译器编译之后，最终产生了多条指令与之对应。

需要指出的是，处理器复位后先是通过频率为几十兆赫兹（MHz）以下的低速设备取指令，例如SPI或LPC等接口。一拍只能取出1比特（SPI）或4比特（LPC），而一条指令一般需要32比特。对于吉赫兹（GHz）的高性能处理器来说，几千拍才能执行一条指令。

### 2. 调试接口初始化

在启动过程中优先初始化的，首先是用于调试的接口部分。比如开机时听到的蜂鸣器响声，或者在一些主板上看到的数码管显示，都是最基本的调试用接口。对于龙芯3号处理器来说，最先初始化的结构是芯片内集成的串口控制器。串口控制器作为一个人机交互的界面，可以提供简单方便的调试手段，以此为基础，再进一步对计算机系统中其他更复杂的部分进行管理。

对串口的初始化操作实际上是处理器对串口执行一连串约定好的IO操作。在X86结构下，IO地址空间与内存地址空间相互独立，IO操作与访存操作是通过不同的指令实现的。MIPS和LoongArch等结构并不显式区分IO和内存地址，而是采用全局编址，使用地址空间将IO和内存隐式分离，并通过地址空间或TLB映射对访问方式进行定序及缓存等的控制。只有理解IO与内存访问的区别，才能很好地理解计算机启动中的各种初始化过程。

内存空间对应的是存储器，存储器不会发生存储内容的自行更新。也就是说，如果处理器核向存储单元A中写入了0x5a5a的数值，除非有其他的主控设备（例如其他的处理器核或是其他的设备DMA）对它也进行写入操作，否则这个0x5a5a的数值是不会发生变化的。

IO空间一般对应的是控制寄存器或状态寄存器，是受IO设备的工作状态影响的。此时，写入的数据与读出的数据可能会不一致，而多次读出的数据也可能不一致，其读出数据是受具体设备状态影响的。例如，对串口的线路状态寄存器（寄存器偏移0x5）的读取在不同的情况下会产生不同的返回值。该寄存器定义如下表7.1所示。

<img src="计算机硬件结构.assets/表7.1 串口线路状态寄存器定义.png" style="zoom:50%;" />

可以看到这个寄存器里的各个数据位都与当时的设备状态相关。例如当程序等待外部输入数据时，就需要查询这个寄存器的第0位，以确定是否收到数据，再从FIFO寄存器中读取实际的数据。在这个轮询的过程中，寄存器的第0位根据串口的工作状态由0变成1。此外，这个寄存器的某些位在读操作之后会产生自动清除的效果，例如第7位（错误表示位）在一次读操作之后会自动清零。

从这个寄存器上可以看到IO访问与内存访问的一些区别。IO寄存器的行为与具体的设备紧密相关，每种IO设备都有各自不同的寄存器说明，需要按照其规定的访问方式进行读写，而不像内存可以进行随意的读写操作。

前面提到，在LoongArch结构下，IO地址空间与内存地址空间统一编址，所以要想办法体现IO操作和内存操作的差异。处理器上运行的指令使用虚拟地址，虚拟地址通过地址映射规则与物理地址相关联，基本的虚拟地址属性首先区分为经缓存（Cache）与不经缓存（Uncache）两种。Cache在离处理器更近的位置上利用访存局部性原理进行缓存，以加速重复访存或者其他规则访存（通过预取等手段）。对于内存操作，现代高性能通用处理器都采用Cache方式进行访问，以提升访存性能。对于存储器来说，在Cache中进行缓存是没有问题的，因为存储器所存储的内容不会自行修改（但可能会被其他核或设备所修改，这个问题可以通过缓存一致性协议解决）。但是对于IO设备来说，因为其寄存器状态是随着工作状态的变化而变化的，如果缓存在Cache中，那么处理器核将无法得到状态的更新，所以一般情况下不能对IO地址空间进行Cache访问，需要使用Uncache访问。使用Uncache访问对IO进行操作还有另一个作用，就是可以严格控制读写的访问顺序，不会因为预取类的操作导致寄存器状态的丢失。例如前面提到的线路状态寄存器的第7位（ERROR），一旦被预取的读操作所访问就会自动清除，而这个预取操作本身有可能会因为错误执行而被流水线取消，这样就导致这个错误状态的丢失，无法被软件观察到。

理解了IO操作与内存访问操作的区别，串口初始化的过程就显得非常简单。串口初始化程序仅仅是对串口的通信速率及一些控制方法进行设置，以使其很方便地通过一个串口交互主机进行字符的读写交互。串口初始化的汇编代码和说明如下。对于串口设备各个寄存器的具体含义，感兴趣的读者可以在相关处理器的用户手册上查找。

```assembly
LEAF (initserial)
li   a0, GS3_UART_BASE  ; 加载串口设备基地址
li   t1, 128            ; 线路控制寄存器，写入0x80（128）表示后续的寄存器访问为分频
sb.b t1, a0, 3          ; 寄存器访问
; 配置串口波特率分频，当串口控制器输入频率为33MHz，并将串口通信速率设置为115200时，分频方式为33000000/16/0x12=114583
; 由于串口通信有固定的起始格式，能够容忍传输两端一定的速率差异，只要将传输两端的速率保持在一定的范围之内就可以保证传输的正确性
li   t1, 0x12
sb.b t1, a0, 0
li   t1, 0x0
sb.b t1, a0, 1
li   t1, 3
sb.b t1, a0, 3          ; 设置传输字符宽度为8，同时将后续的寄存器访问设置为正常寄存器
li   t1, 0
sb.b t1, a0, 1          ; 不使用中断模式
li   t1, 71
sb   t1, a0, 2
jirl ra
END (initserial)
```

这里有一个值得注意的地方，串口设备使用相同的地址映射了两套功能完全不同的寄存器，通过线路控制寄存器的最高位（就是串口寄存器中偏移为3的寄存器的最高位）进行切换。因为其中一套寄存器主要用于串口波特率的设置，只需要在初始化时进行访问，在正常工作状态下完全不用再次读写，所以能够将其访问地址与另一套正常工作用的寄存器相复用来节省地址空间。下表7.2中是两组不同寄存器的定义。

<img src="计算机硬件结构.assets/表7.2 串口的部分地址复用寄存器.png" style="zoom:50%;" />

在初始化时，代码中先将0x3偏移寄存器的最高位设置为1，以访问分频设置寄存器，按照与连接设备协商好的波特率和字符宽度，将初始化信息写入配置寄存器中。然后退出分频寄存器的访问模式，进入正常工作模式。在使用时，串口的对端是一个同样的串口，两个串口的发送端和接收端分别对连，通过双向的字符通信来实现被调试机的字符输出和字符输入功能。

在正常工作模式下，当CPU需要通过串口对外发送和接收字符时，执行的两个函数分别如下。

```assembly
; 字符输出
LEAF (tgt_putchar)
	dli   a1, GS3_UART_BASE  ; 加载串口设备基地址
label:
	ld.bu a2, a1, 0x5        ; 读取线路状态寄存器中的发送FIFO空标志
	andi  a2, a2, 0x20
	beqz  a2, label          ; FIFO非空时等待
	st.b  a0, a1, 0          ; FIFO空时将通过a0传入的字符写入数据寄存器
	jirl  zero, ra, 0
END (tgt_putchar)
```

```assembly
; 字符输入
LEAF (tgt_getchar)
	dli   a0, GS3_UART_BASE  ; 加载串口设备基地址
label:
	ld.bu a1, a0, 0x5        ; 读取线路状态寄存器中的接收FIFO有效标志
	andi  a1, a1, 0x1
	beqz  a1, label          ; 接收FIFO为空时等待
	ld.b  a0, a0, 0          ; FIFO非空时将数据读出并放在a0寄存器中返回
	jirl  zero, ra, 0
END (tgt_getchar)
```

可以看到，串口通过数据FIFO作为软件数据接口，并通过线路状态寄存器中的特定位来表示串口设备的工作状态。串口驱动函数通过观察状态确定是否能够进行数据的输入输出交互。

对于字符输出，串口控制器实现的功能是将发送FIFO中的数据转换为协议的格式并按位通过tx引脚向外发送，再按照发送FIFO的空满状态设置对应的状态寄存器。对于字符输入，串口控制器实现的功能是将在rx引脚上收到的信号通过协议格式进行解析，将解析得到的字符写入接收FIFO，并按照接收FIFO的空满状态设置对应的状态寄存器。

串口是一个功能非常简单的设备，通过硬件提供底层支持，软件进行配合驱动来实现整个字符输入输出功能。再上到应用层面，还需要更多的软件参与。例如，当通过上位机的minicom或其他的串口工具对被调试机进行字符输入时，我们看到自己输入的字符立即显示在minicom界面上，看起来就像是键盘输入给了minicom，minicom显示后通过串口进行发送，但其真正的过程却更为复杂：

1. 用户在上位机的minicom界面中敲击键盘，输入字符A；
2. 上位机的内核通过其键盘驱动获得字符A；
3. 上位机的内核将字符A交给minicom进程；
4. minicom进程调用串口驱动发送字符A；
5. 内核中的串口驱动将字符A通过串口发送给被调试机；
6. 被调试机的软件发现串口接收FIFO状态非空并接收字符A；
7. 被调试机将接收的字符A通过发送函数写入串口发送FIFO；
8. 被调试机的串口将发送FIFO中的字符A发送给上位机；
9. 上位机发现串口接收FIFO状态非空并接收字符A；
10. 上位机将接收的字符A交给minicom进程，minicom将其显示在界面上。

从CPU对串口的初始化过程可以看出，当load与store指令访问IO设备时，与访问内存“直来直去”的行为是完全不同的。

### 3. TLB初始化

TLB作为一个地址映射的管理模块，主要负责操作系统里用户进程地址空间的管理，用以支持多用户多任务并发。然而在处理器启动的过程中，处理器核处于特权态，整个BIOS都工作在统一的地址空间里，并不需要对用户地址空间进行过多干预。此时TLB的作用更多是地址转换，以映射更大的地址空间供程序使用。下面具体来分析TLB在这一过程中的作用。

LoongArch结构采用了分段和分页两种不同的地址映射机制。分段机制将大段的地址空间与物理地址进行映射，具体的映射方法在BIOS下使用窗口机制进行配置，主要供系统软件使用。而分页机制通过TLB起作用，主要由操作系统管理，供用户程序使用。BIOS一般映射两段，其中0x90000000 00000000开始的地址空间被映射为经缓存的地址，0x80000000 00000000开始的地址空间被映射为不经缓存的地址；根据地址空间的转换规则，这两段转换为物理地址时直接抹除地址的高位，分别对应整个物理地址空间，仅仅在是否经过Cache缓存上有所区别。

由于分段机制是通过不同的虚拟地址来映射全部的物理地址空间，并不适合用作用户程序的空间隔离和保护，也不适合需要更灵活地址空间映射方法的场合。这些场景下就需要利用TLB机制。早期的处理器或者比较简单的处理器中没有实现硬件初始化TLB的逻辑，在使用之前需要使用软件对TLB进行初始化。TLB的初始化主要是将全部表项初始化为无效项。

初始化为无效项就是将TLB的每项逐一清空，以免程序中使用的地址被未初始化的TLB表项所错误映射。在没有硬件复位TLB逻辑的处理器里，启动时TLB里可能会包含一些残留的或者随机的内容，这部分内容可能会导致TLB映射空间的错误命中。因此在未实现硬件复位TLB的处理器中，需要对整个TLB进行初始化操作。可以利用正常的TLB表项写入指令，例如LoongArch中的TLBWR指令，通过一个循环将TLB中的表项一项一项地写为无效。也可以利用更高效的指令来将所有表项直接写为无效，例如LoongArch中的INVTLB 0x0指令。

以下是使用TLBWR指令来进行TLB初始化的相关代码及相应说明。具体的TLB结构和原理可以参考第三章的介绍。通过下面这段代码可以看到，初始化的过程实际上就是将整个TLB表项清0的过程。需要特别说明的是，在LoongArch架构中，实际上并不需要使用这样的指令来完成这个过程，而可以直接使用INVTLB这一条指令，由硬件完成类似的循环清空操作。

```assembly
LEAF (CPU_TLBClear)
	dli    a3, 0                 ; 循环变量
	dli    a0, (1<<31)|(12<<24)  ; 设置页大小为4K，31位为1表示无效
	li     a2, 64                ; TLB表项数
label:
	csrwr  a0, 0x10              ; 将表项写入编号为0x10的TLBIDX寄存器
	addi.d a0, 1                 ; 增加TLBIDX中的索引号
	addi.d a3, 1                 ; 增加循环变量
	tlbwr                        ; 写TLB表项
	bne    a3, a2, label
	jirl   zero, ra, 0
END (CPU_TLBClear)
```

前面提到过，越来越多的处理器已经实现了在芯片复位时由硬件进行TLB表项的初始化，这样在BIOS代码中可以不用再使用类似的软件初始化流程，比如从龙芯3A2000开始的桌面或服务器用的处理器就不再需要软件初始化，这能够减少所需的启动时间。但是在一些嵌入式类的处理器上还是需要上面提到的软件初始化流程。

### 4. Cache初始化

Cache的引入能够减小处理器执行和访存延迟之间的性能差异，即缓解存储墙的问题。引入Cache结构，能够大大提高处理器的整体运行效率。在系统复位之后，Cache同样也处于一个未经初始化的状态，也就是说Cache里面可能包含残留的或随机的数据，如果不经初始化，对于Cache空间的访问也可能会导致错误的命中。

不同的处理器可能包含不同的Cache层次，各级Cache的容量也可能各不相同。例如龙芯3A1000处理器包含私有一级指令Cache、私有一级数据Cache和共享二级Cache两个层次，而龙芯3A5000处理器则包含私有一级指令Cache、私有一级数据Cache、私有二级替换Cache和共享三级Cache三个层次。在进行Cache初始化时要考虑所有需要的层次。

Cache的组织结构主要包含标签（Tag）和数据（Data）两个部分，Tag用于保存Cache块状态、Cache块地址等信息，Data则保存数据内容。大多数情况下对Cache的初始化就是对Tag的初始化，只要将其中的Cache块状态设置为无效，其他部分的随机数据就不会产生影响。

龙芯3A5000中一级数据Cache的组织如下图7.2所示。其中Tag上的cs位为0表示该Cache块为无效状态，对该Cache的初始化操作就是使用Cache指令将Tag写为0。对应的ECC位会在Tag写入时自动生成，不需要专门处理。

<img src="计算机硬件结构.assets/图7.2 龙芯3A5000的一级数据Cache组织.png" style="zoom:50%;" />

不同Cache层次中Tag的组织结构可能会略有区别，初始化程序也会稍有不同，在此不一一列举。以下仅以龙芯3A处理器内部的一级指令Cache的初始化为例进行说明。

```assembly
LEAF (godson2_cache_init)
	li     a2, (1<<14)    ; 64KB/4路，为Index的实际数量
	li     a0, 0x0        ; a0表示当前的index
label:
	CACOP  0x0, a0, 0x0   ; 对4路Cache分别进行写TAG操作
	CACOP  0x0, a0, 0x1
	CACOP  0x0, a0, 0x2
	CACOP  0x0, a0, 0x3
	addi.d a0, a0, 0x40   ; 每个Cache行大小为64字节
    bne    a0, a2, label
	jr     ra
END (godson2_cache_init)
```

CACOP为LoongArch指令集中定义的Cache指令，定义为`CACOP code, rj, si12`，其中code表示操作的对象和操作的类型，0x0表示对一级指令Cache进行初始化操作（Store Tag），将指定Cache行的Tag写为0，rj用于表示指定的Cache行，si12在这个操作中表示不同的Cache路数。

需要特别指出的是，上述程序中的Cache指令为特权态指令，只有运行在特权态时，处理器才可以执行Cache指令，这样可以避免用户程序利用某些Cache指令对Cache结构进行破坏。处理器在复位完成之后就处于最高特权态中，完成各项初始化。在加载操作系统执行之后，在操作系统中才会使用用户态对程序的运行加以限制，以防止不同用户进程之间的相互干扰。

在完成所有Cache层次的初始化之后，就可以跳转到Cache空间开始执行，此后程序的运行效率将会有显著的提升。以取指为例，在使用Cache访问之前，需要以指令宽度为单位（龙芯3A5000中为4字节）进行取指操作，在使用Cache访问之后，取指将以Cache行为单位（龙芯3A5000中为64字节），大大提升了取指的效率。

既然Cache的使用能够大大提高程序运行效率，但是并不能先对Cache进行初始化。这是因为，在跳转到Cache空间执行后，虽然程序运行效率大大提升，但随之而来的是处理器内各种复杂猜测机制的使用。例如对取数操作的猜测执行可能导致一个落在TLB映射空间的访存操作，如果此时TLB尚未完成初始化，就可能会导致TLB异常的发生，而TLB异常处理机制的缺失又会导致系统的崩溃。

实际上，在跳转到Cache空间执行前，BIOS中还会对一些处理器具体实现中的细节或功能进行初始化，在保证执行基本安全的状态下，才会跳转到Cache空间执行。这些初始化包括对各种地址窗口的设置、对一些特殊寄存器的初始化等。感兴趣的读者可以自行阅读相关的BIOS实现代码，在此不再赘述。

随着技术进步，片上Cache的容量越来越大，由此却带来了初始化时间越来越长的问题。但同时，在拥有越来越多可用片上资源的情况下，TLB、Cache等结构的初始化也更多地开始使用硬件自动完成，软件需要在这些初始化上耗费的时间也越来越少。例如从龙芯3A2000开始，片上集成的TLB、各级Cache都已经在复位之后由专用的复位电路进行初始化，不再由低效的Uncache程序来完成，大大缩短了系统启动时间。

完成Cache空间的初始化并跳转至Cache空间运行也标志着处理器的核心部分，或者说体系结构相关的初始化部分已经基本完成。接下来将对计算机系统所使用的内存总线和IO总线等外围部分进行初始化。如果把CPU比作一个大房间，完成对TLB、Cache等的初始化后，房间内已是灯火通明，但大门（内存接口）和窗口（IO接口）还是紧闭的。

## （二）总线接口初始化

在使用同一款处理器的不同系统上，TLB、Cache这些体系结构紧密相关的芯片组成部分的初始化方法是基本一致的，不需要进行特别的改动。与此不同的是，内存和IO设备的具体组成在计算机系统中则可以比较灵活地搭配，不同系统间的差异可能会比较大。在计算机系统硬件设计中，内存可以使用不同的总线类型，例如DDR、DDR2、DDR3或者DDR4。在主板上使用时，也可以灵活地增加或者减小内存容量，甚至改变内存条种类，例如将非缓冲型内存模组（Unbuffered DIMM，UDIMM）改为寄存型内存模组（Registered DIMM，RDIMM）。

IO总线的情况也比较类似，在计算机系统硬件设计中可以搭配不同的桥片，也可以在主板上根据实际情况改变某些接口上连接的设备，例如增加PCIE网卡或者增加硬盘数量等。这些不同的配置情况要求在计算机系统启动时能够进行有针对性的初始化。前面提到过，初始化就是将某些寄存器通过load/store指令或是其他的方法设置为期望数值的过程。

### 1. 内存初始化

内存是计算机系统的重要组成部分。冯诺依曼结构下，计算机运行时的程序和数据都被保存在内存之中。相对复位时用于取指的ROM或是Flash器件来说，内存的读写性能大幅提高。以SPI接口的Flash为例，即使不考虑传输效率的损失，当SPI接口运行在50MHz时，其带宽最高也只有$50\text{MHz}\times 2\text{b}=100\text{Mb/s}$，而一个DDR3-1600的接口在内存宽度为64位时，其带宽可以达到最高$1600\text{MHz}\times 64\text{b}=102400\text{Mb/s}$。由此可见内存的使用对系统性能的重要程度。

越来越多的处理器已经集成内存控制器。因为内存的使用和设置与外接内存芯片的种类、配置相关，所以在计算机系统启动的过程中需要先获取内存的配置信息，再根据该信息对内存控制器进行配置。正如上一章对内存总线的介绍，这些信息包含内存条的类型、容量、行列地址数、运行频率等。获取这些信息是程序通过I2C总线对外部内存条的SPD芯片进行读操作来完成的。SPD芯片也相当于一个Flash芯片，专门用于存储内存条的配置信息。

以下代码段就是龙芯3A处理器上对内存控制器进行初始化的程序片段。

```assembly
ddr2_config:
	add.d  a2, a2, s0         ; a2、s0为调用该程序时传入的参数，它们的和表示初始化参数在Flash中的基地址
	dli    t1, DDR_PARAM_NUM  ; t1用于表示内存参数的个数
	addi.d v0, t8, 0x0        ; t8为调用该程序时传入的参数，用于表示内存控制器的寄存器基地址
label:
	ld.d   a1, 0x0(a2)        ; 初始化的过程就是从Flash中取数再写入内存控制器中的寄存器的过程
	st.d   a1, 0x0(v0)
	addi.d t1, t1, -1
	addi.d a2, a2, 0x8
	addi.d v0, v0, 0x8
	bnez   t1, label
```

如上面的程序片段所示，对内存的初始化实际上就是根据内存配置信息对内存控制器进行初始化。与Cache初始化类似的是，内存初始化并不涉及其存储的初始数据；与Cache又有所不同的地方在于，Cache有专门的硬件控制位来表示Cache块是否有效，而内存却并不需要这样的硬件控制位。内存的使用完全是由软件控制的，软件知道其访问的每一个地址是否存在有效的数据；而Cache是一个硬件加速部件，大多数情况下软件并不需要真正知道而且也不希望知道其存在，Cache的硬件控制位正是为了掩盖内存访问延迟，保证访存操作的正确性。因此内存初始化仅仅需要对内存控制器进行初始化，并通过控制器对内存的状态进行初始化。在初始化完成以后，如果是休眠唤醒，程序可以使用内存中已有的数据来恢复系统状态；如果是普通开机，则程序可以完全不关心内存数据而随意使用。

内存控制器的初始化包括两个相对独立的部分，一是根据内存的行地址、列地址等对内存地址映射进行配置，二是根据协议对内存信号调整的支持方式，对内存读写信号相关的寄存器进行训练，以保证传输时的数据完整性。在内存初始化完成后，可能还需要根据内存的大小对系统可用的物理地址空间进行相应的调整和设置。

### 2. IO总线初始化

前面提到，受外围桥片搭配及可插拔设备变化的影响，系统每次启动时需要对IO总线进行有针对性的初始化操作。

对于龙芯3A5000处理器，对应的IO总线主要为HyperTransport总线。在IO总线初始化时，需要做三件事：一是对IO总线的访问地址空间进行设置，划定设备的配置访问空间、IO访问空间和Memory访问空间；二是对IO设备的DMA访问空间进行规定，对处理器能接收的DMA内存地址进行设置；三是对HyperTransport总线进行升频，从复位时1.0模式的200MHz升频到3.0模式的2.0GHz，并将总线宽度由8位升至16位。完成这三件事，对IO总线的初始化就已基本完成。接着还将设置一些和桥片特性相关的配置寄存器以使桥片正常工作。

IO总线初始化的主要目的是将非通用的软硬件设置都预先配置好，将与桥片特性相关的部分与后面的标准驱动加载程序完全分离出来，以保证接下来的通用设备管理程序的运行。如果把CPU比作一个大房间，至此，房间灯火通明，门窗均已打开，但门窗外还是漆黑一片。

完成内存与IO总线初始化后，BIOS中基本硬件初始化的功能目标已经达到。但是为了加载操作系统，还必须对系统中的一些设备进行配置和驱动。操作系统所需要的存储空间比较大，通常无法保存在Flash这样的存储设备中，一般保存在硬盘中并在使用时加载，或者也可以通过网口、U盘等设备进行加载。为此就需要使用更复杂的软件协议来驱动系统中的各种设备，以达到加载操作系统的最终目标。

在此之前的程序运行基本没有使用内存进行存数取数操作，程序也是存放在Flash空间之中的，只不过是经过了Cache的缓存加速。在此之后的程序使用的复杂数据结构和算法才会对内存进行大量的读写操作。为了进一步提高程序的运行效率，先将程序复制到内存中，再跳转到内存空间开始执行。

此后，还需要对处理器的运行频率进行测量，对BIOS中的计时函数进行校准，以便在需要等待的位置进行精确的时间同步。在经过对各种软件结构必要的初始化之后，BIOS将开始一个比较通用的设备枚举和驱动加载的过程。下一节将对这个标准的设备枚举和加载过程进行专门的说明。

## （三）设备的探测及驱动加载

PCI总线于20世纪90年代初提出，发展到现在已经逐渐被PCIE等高速接口所替代，但其软件配置结构却基本没有发生变化，包括HyperTransport、PCIE等新一代高速总线都兼容PCI协议的软件框架。在PCI软件框架下，系统可以灵活地支持设备的自动识别和驱动的自动加载。下面对PCI的软件框架进行简要说明。

在PCI协议下，IO的系统空间分为三个部分：配置空间、IO空间和Memory空间。配置空间存储设备的基本信息，主要用于设备的探测和发现；IO空间比较小，用于少量的设备寄存器访问；Memory空间可映射的区域较大，可以方便地映射设备所需要的大块物理地址空间。

对于X86架构来说，IO空间的访问需要使用IO指令操作，Memory空间的访问则需要使用通常的load/store指令操作。而对于MIPS或者LoongArch这种把设备和存储空间统一编址的体系结构来说，IO空间和Memory空间没有太大区别，都使用load/store指令操作。IO空间与Memory空间的区别仅在于所在的地址段不同，对于某些设备的Memory访问，可能可以采用更长的单次访问请求。例如对于IO空间，可以限制为仅能使用字访问，而对于Memory空间，则可以任意地使用字、双字甚至更长的Cache行访问。

配置空间的地址偏移由总线号、设备号、功能号和寄存器号的组合得到，通过对这个组合的全部枚举，可以很方便地检测到系统中存在的所有设备。

以HyperTransport总线为例，配置访问分为两种类型，即Type0和Type1，其区别在于基地址和对总线号的支持。如下图7.3所示，只需要在图中总线号、设备号、功能号的位置上进行枚举，就可以遍历整个总线，检测到哪个地址上存在设备。

<img src="计算机硬件结构.assets/图7.3 HyperTransport总线配置访问的两种类型.png" style="zoom:50%;" />

通过这种方式，即使在某次上电前总线上的设备发生了变化，也可以在这个枚举的过程中被探测到。而每个设备都拥有唯一的识别号，即下图7.4中的设备号和厂商号，通过加载这些识别号对应的驱动，就完成了设备的自动识别和驱动的自动加载。

<img src="计算机硬件结构.assets/图7.4 标准的设备配置空间寄存器分布.png" style="zoom:50%;" />

图7.4是标准的设备配置空间寄存器分布，对于所有设备，这个空间的分布都是一致的，以保证PCI协议对其进行统一的检索。图中的厂商识别号（Vendor ID）与设备识别号（Device ID）的组合是唯一的，由专门的组织进行管理。每一个提供PCI设备的厂商都应该拥有唯一的厂商识别号，以在设备枚举时正确地找到其对应的驱动程序。例如英特尔的厂商识别号为0x8086，龙芯的厂商识别号为x0014。设备识别号对于每一个设备提供商的设备来说应该是唯一的。这两个识别号的组合就可以在系统中唯一地指明正确的驱动程序。

除了通过厂商识别号与设备识别号对设备进行识别并加载驱动程序之外，还可以通过设备配置空间寄存器中的类别代码（Class Code）对一些通用的设备进行识别，并加载通用驱动。例如USB接口所使用的OHCI（Open Host Controller Interface）用于USB2.0 Full Speed或其他接口，EHCI（Enhanced Host Controller Interface），用于USB2.0 High Speed，XHCI（eXtensible Host Controller Interface）用于USB3.0，SATA接口所使用的AHCI（Advance Host Controller Interface）用于SATA接口等。这一类通用接口控制器符合OHCI、EHCI、XHCI或AHCI规范所规定的标准接口定义和操作方法，类似于处理器的指令集定义，只要符合相应的规范，即使真实的设备不同，也能够运行标准的驱动程序。

所谓驱动程序就是一组函数，包含用于初始化设备、关闭设备或是使用设备的各种相关操作。还是以最简单的串口设备为例，如果在设备枚举时找到了一个PCI串口设备，它的驱动程序里面可能包含下面几类函数。首先是初始化函数，在找到设备后，首先执行一次初始化函数，以使设备到达可用状态。然后是发送数据函数和接收数据函数。在Linux内核中，系统通过调用读写函数接口实现真正的设备操作。在发送数据函数和接收数据函数中，需要将设备发送数据和接收数据的方法与读写函数的接口相配合，这样在系统调用串口写函数时，能够通过串口发送数据，调用串口读函数时，能够得到串口接收到的数据。此外还有中断处理函数，当串口中断发生时，让中断能够进入正确的处理函数，通过读取正确的中断状态寄存器，找到中断发生的原因，再进行对应的处理。

当然，为了实现所有设备的共同工作，还需要其他PCI协议特性的支持。首先就是对于设备所需IO空间和Memory空间的灵活设置。从图7.4可以看到，在配置空间中，并没有设备本身功能上所使用的寄存器，这些寄存器实际上是由可配置的IO空间或Memory空间来索引的。图7.4的配置空间中存在6组独立的基址寄存器（Base Address Register，BAR），这些BAR一方面用于告诉软件该设备所需要的地址空间类型及其大小，另一方面用于接收软件给其配置的基地址。

BAR的寄存器定义如图7.5所示，其最低位表示该BAR是IO空间还是Memory空间。BAR中间有一部分只读位为0，正是这些0的个数表示该BAR所映射空间的大小，也就是说BAR所映射的空间为2的幂次方大小。BAR的高位是可写位，用来存储软件设置的基地址。

<img src="计算机硬件结构.assets/图7.5 BAR的寄存器定义.png" style="zoom:50%;" />

在这种情况下，对一个BAR的基地址配置方式首先是确定BAR所映射空间的大小，再分配一个合适的空间，给其高位基地址赋值。确定BAR空间大小的方法也很巧妙，只要给这个寄存器先写入全1的值，再读出来观察0的个数即可得到。

对PCI设备的探测和驱动加载是一个递归调用过程，大致算法如下：

1. 将初始总线号、初始设备号、初始功能号设为0。
2. 使用当前的总线号、设备号、功能号组成一个配置空间地址，这个地址的构成如图7.3所示，使用该地址，访问其0号寄存器，检查其设备号。
3. 如果读出全1或全0，表示无设备。
4. 如果该设备为有效设备，检查每个BAR所需的空间大小，并收集相关信息。
5. 检测其是否为一个多功能设备，如果是则将功能号加1再重复扫描，执行第2步。
6. 如果该设备为桥设备，则给该桥配置一个新的总线号，再使用该总线号，从设备号0、功能号0开始递归调用，执行第2步。
7. 如果设备号非31，则设备号加1，继续执行第2步；如果设备号为31，且总线号为0，表示扫描结束，如果总线号非0，则退回上一层递归调用。

通过这个递归调用，就可以得到整个PCI总线上的所有设备及其所需要的所有空间信息。有了这些信息，就可以使用排序的方法对所有的空间从大到小进行分配。最后，利用分配的基地址和设备的ID信息，加载相应的驱动就能够正常使用该设备。

下面是从龙芯3A处理器PCI初始化代码中抽取出的程序片段，通过这个片段，可以比较清楚地看到整个软件处理过程。

```c
void _pci_businit(int init) {
    /* ... */
    // 这里的pci_roots用于表示系统中有多少个根节点，通常的计算机系统中都为1
    for(i = 0, pb = pci_head; i < pci_roots; i++, pb = pb->next) {
        _pci_scan_dev(pb, i, 0, init);
    }
    /* ... */
    _setup_pcibuses(init);  // 对地址窗口等进行配置
}
static void _pci_scan_dev(struct pci_pdevice *dev, int bus, int device, int initialise) {
    for( ; device < 32; device++) {
        // 对本级总线，扫描所有32个设备位置，判断是否存在设备
        _pci_query_dev(dev, bus, device, initialize);
    }
}
static void _pci_query_dev(struct pci_device *dev, int bus, int device, int initialise) {
    /* ... */
    misc = _pci_conf_read(tag, PCI_BHLC_REG);
    // 检测是否为多功能设备
    if(PCI_HDRTYPE_MULTIFN(misc)) {
        for(function = 0; function < 8; function++) {
            tag = _pci_make_tag(bus, device, function);
            id = _pci_conf_read(tag, PCI_ID_REG);
            if(id == 0 || id == 0xFFFFFFFF) {
                continue;
            }
            _pci_query_dev_func(dev, tag, initialise);
        }
    } else {
        _pci_query_dev_func(dev, tag, initialise);
    }
}
void _pci_query_dev_func(struct pci_device *dev, pcitag tag, int initialise) {
    /* ... */
    class = _pci_conf_read(tag, PCI_CLASS_REG);  // 读取配置头上的类别信息
    id = _pci_conf_read(tag, PCI_ID_REG);  // 读取配置头上的厂商ID、设备ID
    /* ... */
    // 对于桥设备，需要递归处理下级总线
    if(PCI_ISCLASS(class, PCI_CLASS_BRIDGE, PCI_SUBCLASS_BRIDGE_PCI)) {
        /* ... */
        pd->bridge.pribus_num = bus;  // 设置桥上的总线号信息
        pd->bridge.secbus_num = ++_pci_nbus;
        /* ... */
        _pci_scan_dev(pd, pd->bridge.secbus_num, 0, initialise);  // 开始递归调用
        /* ... */
        /* 收集整个下级总线所需要的资源信息 */
    } else {
        /* 收集本设备所需要的资源信息 */
    }
}
```

假设Memory空间的起始地址为0x40000000，在设备扫描过程中发现了USB控制器、显示控制器和网络控制器，三个设备对于Memory空间的需求如下表7.3所示。

<img src="计算机硬件结构.assets/表7.3 三个设备的空间需求.png" style="zoom:50%;" />

在得到以上信息后，软件对各个设备的空间需求进行排序，并依次从Memory空间的起始地址开始分配，最终得到的设备地址空间分布如下表7.4所示。

<img src="计算机硬件结构.assets/表7.4 三个设备的地址空间分布.png" style="zoom:50%;" />

经过这样的设备探测和驱动加载过程，可以将键盘、显卡、硬盘或者网卡等设备驱动起来，在这些设备上加载预存的操作系统，就完成了整个系统的正常启动。如果把CPU比作一个大房间，至此，房间内灯火通明，门窗均已打开，门窗外四通八达。CPU及相关硬件处于就绪状态。

## （四）多核启动过程

上面几节主要讨论了从处理器核初始化、总线初始化、外设初始化到操作系统加载的启动过程。启动过程中多处理器核间的相互配合将在本节进行讨论。

实现不同处理器核之间相互同步与通信的一种手段是核间中断与通信信箱机制。在龙芯3号处理器中，为每个处理器核实现了一组核间通信寄存器，包括一组中断寄存器和一组信箱寄存器。这组核间通信寄存器也属于IO寄存器的一种。实际上，信箱寄存器完全可以通过在内存中分配一块地址空间实现，这样CPU访问延迟更短。而专门使用寄存器实现的信箱寄存器更多是为了在内存还没有初始化前就让不同的核间能够有效通信。

### 1. 初始化时的多核协同

在BIOS启动过程中，为了简化处理流程，实际上并没有用到中断寄存器，对于各种外设也没有使用中断机制，都是依靠处理器的轮询来实现与其他设备的协同工作。

为了简化多核计算机系统的启动过程，我们将多核处理器中的一个核定为主核，其他核定为从核。主核除了对本处理器核进行初始化之外，还要负责对各种总线及外设进行初始化；而从核只需要对它自己处理器核的私有部件进行初始化，之后在启动过程中就可以保持空闲状态，直至进入操作系统再由主核进行调度。

从核需要初始化的工作包括以下几个部分。首先是从核私有的部分，所谓私有，就是其他处理器核无法直接操纵的部件，例如核内的私有寄存器、TLB、私有Cache等，这些器件只能由每个核自己进行初始化而无法由其他核代为进行。其次还有为了加速整个启动过程，由主核分配给从核做的工作，例如当共享Cache的初始化操作非常耗时的时候，可以将整个共享Cache分为多个部分，由不同的核负责某一块共享Cache的初始化，通过并行处理的方式进行加速。

主核的启动过程与前三节介绍的内容基本是一致的。但在一些重要的节点上则需要与从核进行同步与通信，或者说通知从核系统已经到达了某种状态。为了实现这种通知机制，可以将信箱寄存器中不同的位定义为不同的含义，一旦主核完成了某些初始化阶段，就可以给信箱寄存器写入相应的值。例如将信箱寄存器的第0位定义为“串口初始化完成”标志，第1位定义为“共享Cache初始化完成”标志，第2位定义为“内存初始化完成”标志。

在主核完成串口的初始化后，可以向自己的信箱寄存器写入0x1。从核在第一次使用串口之前需要查询主核的信箱寄存器，如果第0位为0，则等待并轮询，如果非0，则表示串口已经初始化完成，可以使用。在主核完成了共享Cache的初始化后，向自己的信箱寄存器写入0x3。而从核在初始化自己的私有Cache之后，还不能直接跳转到Cache空间执行，必须等待信号，以确信主核已将全部的共享Cache初始化完成，然后再开始Cache执行才是安全的。在主核完成了内存初始化后，其他核才能使用内存进行数据的读写操作。那么从核在第一次用到内存之前就必须等待表示内存初始化完成的0x7标志。

### 2. 操作系统启动时的多核唤醒

当从核完成了自身的初始化之后，如果没有其他工作需要进行，就会跳转到一段等待唤醒的程序。在这个等待程序里，从核定时查询自己的信箱寄存器。如果为0，则表示没有得到唤醒标志。如果非0，则表示主核开始唤醒从核，此时从核还需要从其他几个信箱寄存器里得到唤醒程序的目标地址，以及执行时的参数。然后从核将跳转到目标地址开始执行。

以下为龙芯3A5000的BIOS中从核等待唤醒的相关代码。

```assembly
slave_main:
	; NODE0_CORE0_BUF0为0号核的信箱寄存器地址，其他核的信箱寄存器地址与之相关
	; 在此根据主核的核号，确定主核信箱寄存器的实际地址
	dli   t2, NODE0_CORE0_BUF0
	dli   t3, BOOTCORE_ID
	sll.d t3, 8
	or    t2, t2, t3
	
wait_scache_allover:
	ld.w  t4, t2, FN_OFF     ; 等待主核写入初始化完成标志
	dli   t5, SYSTEM_INIT_OK
	bne   t4, t5, wait_scache_allover
	bl    clear_mailbox      ; 对每个核各自的信箱寄存器进行初始化

waitforinit:
	li    a0, 0x1000
idle1000:
	addiu a0, -1
	bnez  a0, idle1000
	ld.w  t2, t1, FN_OFF     ; t1为各个核的信箱寄存器地址，轮询等待
	beqz  t2, waitforinit
	ld.d  t2, t1, FN_OFF     ; 通过读取低32位确定是否写入，再读取64位得到完整地址
	ld.d  sp, SP_OFF(t1)     ; 从信箱寄存器中的其他地方取回相关启动参数
	ld.d  gp, GP_OFF(t1)
	ld.d  a1, A1_OFF(t1)
	move  ra, t2             ; 转至唤醒地址，开始执行
	jirl  zero, ra, 0x0
```

在操作系统中，主核在各种数据结构准备好的情况下就可以开始依次唤醒每一个从核。唤醒的过程也是串行的，主核唤醒从核之后也会进入一个等待过程，直到从核执行完毕再通知主核，再唤醒一个新的从核，如此往复，直至系统中所有的处理器核都被唤醒并交由操作系统管理。

### 3. 核间同步与通信

操作系统启动之前，利用信箱寄存器进行了大量的多核同步与通信操作，但在操作系统启动之后，除了休眠唤醒一类的操作，却基本不会用到信箱寄存器。Linux内核中，只需要使用核间中断就可以完成各种核间的同步与通信操作。

核间中断也是利用一组IO寄存器实现的。通过将目标核的核间中断寄存器置1来产生一个中断信号，使目标核进入中断处理。中断处理的具体内容则是通过内存进行交互的。内核中为每个核维护一个队列（内存中的一个数据结构），当一个核想要中断其他核时，它将需要处理的内容加入目标核的这个队列，然后再向目标核发出核间中断（设置其核间中断寄存器）。当目标核被中断之后，开始处理其核间通信队列，如果其间还收到了更多的核间中断请求，也会一并处理。

Linux内核中的核间中断处理不通过信箱寄存器进行的原因有以下几点。首先信箱寄存器只有一组，也就是说如果通过信箱寄存器发送通信消息，在这个消息没被处理之前，是不能有其他核再向其发出新的核间中断的，这样无疑会导致核间中断发送方的阻塞。另外，这组核间通信寄存器实际上是IO寄存器，前面提到，对于IO寄存器的访问是通过不经缓存这种严格访问序的方式进行的，相比于Cache访问方式，不经缓存读写效率极其低下，本身延迟开销很大，还可能会导致流水线的停顿。因此在实际的内核中，只有类似休眠唤醒这种特定的同步操作才会利用信箱寄存器进行，其他的同步通信操作则是利用内存传递信息，并利用核间中断寄存器产生中断的方式共同完成的。
